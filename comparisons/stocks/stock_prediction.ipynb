{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Value Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import pandas as pd\n",
    "\n",
    "from flax import linen\n",
    "from stockdex import Ticker\n",
    "from turbanet import TurbaTrainState, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOW = 20\n",
    "HIDDEN_SIZE = 32\n",
    "EPOCHS = 10_000\n",
    "LR = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWARM_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in stock data\n",
    "df = pd.read_csv(\"../../data/input/stock_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences of price data of length TIME_WINDOW\n",
    "def create_sequences(data, time_window) -> tuple[np.ndarray, np.ndarray]:\n",
    "    sequences = []\n",
    "    results = []\n",
    "    for i in range(len(data) - time_window):\n",
    "        sequence = data[i : i + time_window]\n",
    "        sequences.append(sequence)\n",
    "        results.append(data[i + time_window])\n",
    "    return np.array(sequences).reshape(-1, time_window, 1), np.array(results).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker_data(ticker, date_range=\"1y\", data_granularity=\"1d\"):\n",
    "    data = Ticker(ticker=ticker).yahoo_api_price(\n",
    "        range=date_range, dataGranularity=data_granularity\n",
    "    )\n",
    "    close = data[\"close\"]\n",
    "    if close.empty:\n",
    "        return None\n",
    "    close /= np.max(close)\n",
    "    X_data, y_data = create_sequences(close, TIME_WINDOW)\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_ticker_data(stock_df, num_samples, date_range=\"1y\", data_granularity=\"1d\"):\n",
    "    tickers = []\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    max_seq_len = 0\n",
    "    while len(tickers) < num_samples:\n",
    "        ticker = stock_df.sample(1).Ticker.values[0]\n",
    "        try:\n",
    "            ticker_data = get_ticker_data(ticker, date_range, data_granularity)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # If the data returned is None, try again\n",
    "        if ticker_data is None:\n",
    "            continue\n",
    "\n",
    "        if ticker_data[0].shape[0] < max_seq_len:\n",
    "            continue\n",
    "\n",
    "        if ticker_data[0].shape[0] > max_seq_len:\n",
    "            max_seq_len = ticker_data[0].shape[0]\n",
    "            invalid_tickers = [x.shape[0] < max_seq_len for x in X_data]\n",
    "            if len(invalid_tickers) > 0:\n",
    "                del tickers[invalid_tickers]\n",
    "                del X_data[invalid_tickers]\n",
    "                del y_data[invalid_tickers]\n",
    "\n",
    "        tickers.append(ticker)\n",
    "        X_data.append(ticker_data[0])\n",
    "        y_data.append(ticker_data[1])\n",
    "\n",
    "        print(f\"Tickers Found: {len(tickers)}/{num_samples}\")\n",
    "\n",
    "    return tickers, np.array(X_data), np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKERS, X_data, y_data = get_random_ticker_data(\n",
    "    df, SWARM_SIZE, date_range=\"1y\", data_granularity=\"1d\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurbaLSTM(linen.Module):\n",
    "    features: int\n",
    "\n",
    "    @linen.compact\n",
    "    def __call__(self, x):\n",
    "        ScanLSTM = linen.scan(\n",
    "            linen.OptimizedLSTMCell,\n",
    "            variable_broadcast=\"params\",\n",
    "            split_rngs={\"params\": False},\n",
    "            in_axes=1,\n",
    "            out_axes=1,\n",
    "        )\n",
    "\n",
    "        lstm = ScanLSTM(self.features)\n",
    "        input_shape = x[:, 0].shape\n",
    "        carry = lstm.initialize_carry(jax.random.PRNGKey(0), input_shape)\n",
    "        carry, x = lstm(carry, x)\n",
    "        final = x[:, -1]\n",
    "        output = linen.Dense(1)(final)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "turba_model = TurbaTrainState.swarm(TurbaLSTM(features=HIDDEN_SIZE), optimizer, SWARM_SIZE, X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Turba model\n",
    "start = time.time()\n",
    "turba_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    turba_model, loss, pred = turba_model.train(X_data, y_data, mse)\n",
    "\n",
    "    # Logging\n",
    "    turba_losses.append(loss)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch} Losses: {jnp.array(turba_losses[-100:]).mean(axis=0)}\")\n",
    "\n",
    "print(f\"Turba time: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "turba_losses = jnp.array(turba_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "turba_predictions = turba_model.predict(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of losses over training\n",
    "plt.figure(figsize=(32, 16))\n",
    "plt.plot(turba_losses, label=TICKERS)\n",
    "plt.title(\"Loss\", fontsize=32)\n",
    "plt.xlabel(\"Epochs\", fontsize=24)\n",
    "plt.ylabel(\"Loss\", fontsize=24)\n",
    "plt.legend(fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot of predictions vs ground truth (x by x)\n",
    "fig = plt.figure(figsize=(32, 16))\n",
    "\n",
    "# Shared axes\n",
    "for i in range(SWARM_SIZE):\n",
    "    ax = fig.add_subplot(\n",
    "        int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), i + 1\n",
    "    )\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(TICKERS[i])\n",
    "\n",
    "    # Axes\n",
    "    ax.set_ylabel(\"Price\")\n",
    "\n",
    "    # Data\n",
    "    ax.plot(y_data[i], label=\"Ground Truth\")\n",
    "    ax.plot(turba_predictions[i], label=\"Turba Prediction\")\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TorchLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=128):\n",
    "        super(TorchLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(1, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell state (h_0, c_0) with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h_0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))  # Shape: (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Take the last time step output\n",
    "        final_output = out[:, -1, :]  # Shape: (batch, hidden_size)\n",
    "\n",
    "        # Fully connected layer to map hidden state to final output\n",
    "        return self.fc(final_output)  # Shape: (batch, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_models = [TorchLSTM(hidden_size=HIDDEN_SIZE) for _ in range(SWARM_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the PyTorch model\n",
    "start = time.time()\n",
    "\n",
    "torch_loss = torch.nn.MSELoss()\n",
    "torch_losses = []\n",
    "for idx, torch_model in enumerate(torch_models):\n",
    "    torch_model.train()\n",
    "    torch_optimizer = torch.optim.Adam(torch_model.parameters(), lr=LR)\n",
    "\n",
    "    torch_losses.append([])\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Train\n",
    "        torch_optimizer.zero_grad()\n",
    "        y_pred = torch_model(torch.Tensor(X_data[idx]))\n",
    "        loss = torch_loss(y_pred, torch.Tensor(y_data[idx]))\n",
    "        loss.backward()\n",
    "        torch_optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        torch_losses[idx].append(loss.item())\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Model {idx} - Epoch {epoch} Loss: {np.mean(torch_losses[idx][-100:])}\")\n",
    "\n",
    "print(f\"torch time: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_predictions = []\n",
    "for idx, torch_model in enumerate(torch_models):\n",
    "    torch_model.eval()\n",
    "    torch_predictions.append(torch_model(torch.Tensor(X_data[idx])).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_losses = np.array(torch_losses).T\n",
    "torch_predictions = np.array(torch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of losses over training\n",
    "plt.figure(figsize=(32, 16))\n",
    "plt.plot(torch_losses, label=TICKERS)\n",
    "plt.title(\"Loss\", fontsize=32)\n",
    "plt.xlabel(\"Epochs\", fontsize=24)\n",
    "plt.ylabel(\"Loss\", fontsize=24)\n",
    "plt.legend(fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot of predictions vs ground truth (x by x)\n",
    "fig = plt.figure(figsize=(32, 16))\n",
    "\n",
    "# Shared axes\n",
    "for i in range(SWARM_SIZE):\n",
    "    ax = fig.add_subplot(\n",
    "        int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), i + 1\n",
    "    )\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(TICKERS[i])\n",
    "\n",
    "    # Axes\n",
    "    ax.set_ylabel(\"Price\")\n",
    "\n",
    "    # Data\n",
    "    ax.plot(y_data[i], label=\"Ground Truth\")\n",
    "    ax.plot(torch_predictions[i], label=\"Torch Prediction\")\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot of predictions vs ground truth (x by x)\n",
    "fig = plt.figure(figsize=(32, 20))\n",
    "\n",
    "# Shared axes\n",
    "for i in range(SWARM_SIZE):\n",
    "    ax = fig.add_subplot(\n",
    "        int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), i + 1\n",
    "    )\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(TICKERS[i])\n",
    "\n",
    "    # Axes\n",
    "    ax.set_ylabel(\"Price\")\n",
    "\n",
    "    # Data\n",
    "    ax.plot(y_data[i], label=\"Ground Truth\")\n",
    "    ax.plot(torch_predictions[i], label=\"Torch Prediction\")\n",
    "    ax.plot(turba_predictions[i], label=\"Turba Prediction\")\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
