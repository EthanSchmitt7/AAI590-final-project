{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Value Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from flax import linen\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stockdex import Ticker\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from turbanet import TurbaTrainState, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Hyperparameters\n",
    "\n",
    "We define key hyperparameters for training our models, here is an example of a configuration:\n",
    "\n",
    "- **EPOCHS = 10,000** → Train for 10,000 full passes over the dataset.\n",
    "- **SWARM_SIZE = 100** → Train 100 models on different tickers concurrently using TurbaNet.\n",
    "- **BATCH_SIZE = 8** → Process 8 sequences per training batch.\n",
    "- **LR = 1e-5** → Learning rate set to `0.00001` for stable convergence.\n",
    "- **CPU = True** → Force PyTorch to use the CPU instead of the GPU. \n",
    "    - This can be helpful for comparing performance of the libraries when running on Windows machines as [Jax does not support GPUs running on Windows](https://docs.jax.dev/en/latest/installation.html).\n",
    "- **TIME_WINDOW = 20** → Sequence length for the input prior to the prediction.\n",
    "- **HIDDEN_SIZE = 32** → Size of the hidden layer of the network.\n",
    "\n",
    "These settings balance **training stability** and **efficiency**, leveraging **swarm-based training** to speed up model convergence.\n",
    "\n",
    "Feel free to tweak this values and examine the impact on training speed and effectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "SWARM_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "BOOTSTRAP_SIZE = None\n",
    "LR = 1e-5\n",
    "CPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOW = 20\n",
    "HIDDEN_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in stock data\n",
    "df = pd.read_csv(\"../../data/input/stock_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences of price data of length TIME_WINDOW\n",
    "def create_sequences(data, time_window) -> tuple[np.ndarray, np.ndarray]:\n",
    "    sequences = []\n",
    "    results = []\n",
    "    for i in range(len(data) - time_window):\n",
    "        sequence = data[i : i + time_window]\n",
    "        sequences.append(sequence)\n",
    "        results.append(data[i + time_window])\n",
    "    return np.array(sequences).reshape(-1, time_window, 1), np.array(results).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker_data(ticker, date_range=\"1y\", data_granularity=\"1d\"):\n",
    "    data = Ticker(ticker=ticker).yahoo_api_price(\n",
    "        range=date_range, dataGranularity=data_granularity\n",
    "    )\n",
    "    close = data[\"close\"]\n",
    "    if close.empty:\n",
    "        return None\n",
    "    close /= np.max(close)\n",
    "    X_data, y_data = create_sequences(close, TIME_WINDOW)\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_ticker_data(stock_df, num_samples, date_range=\"1y\", data_granularity=\"1d\"):\n",
    "    tickers = []\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    max_seq_len = 0\n",
    "    while len(tickers) < num_samples:\n",
    "        ticker = stock_df.sample(1).Ticker.values[0]\n",
    "        try:\n",
    "            ticker_data = get_ticker_data(ticker, date_range, data_granularity)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # If the data returned is None, try again\n",
    "        if ticker_data is None:\n",
    "            continue\n",
    "\n",
    "        if ticker_data[0].shape[0] < max_seq_len:\n",
    "            continue\n",
    "\n",
    "        if ticker_data[0].shape[0] > max_seq_len:\n",
    "            max_seq_len = ticker_data[0].shape[0]\n",
    "            invalid_tickers = [x.shape[0] < max_seq_len for x in X_data]\n",
    "            if len(invalid_tickers) > 0:\n",
    "                del tickers[invalid_tickers]\n",
    "                del X_data[invalid_tickers]\n",
    "                del y_data[invalid_tickers]\n",
    "\n",
    "        tickers.append(ticker)\n",
    "        X_data.append(ticker_data[0])\n",
    "        y_data.append(ticker_data[1])\n",
    "\n",
    "        print(f\"Tickers Found: {len(tickers)}/{num_samples}\", end=\"\\r\", flush=True)\n",
    "\n",
    "    return tickers, np.array(X_data), np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKERS, X_data, y_data = get_random_ticker_data(\n",
    "    df, SWARM_SIZE, date_range=\"2y\", data_granularity=\"1d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tickers_table(tickers, cols=5):\n",
    "    table = Table(title=\"Stock Tickers\", show_header=False)\n",
    "    for i in range(0, len(tickers), cols):\n",
    "        table.add_row(*tickers[i : i + cols], *[\"\"] * (cols - len(tickers[i : i + cols])))\n",
    "\n",
    "    console = Console()\n",
    "    console.print(table)\n",
    "\n",
    "\n",
    "display_tickers_table(TICKERS, cols=np.sqrt(len(TICKERS)).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "if BATCH_SIZE is None:\n",
    "    BATCH_SIZE = X_data.shape[1]\n",
    "X_data = X_data.transpose((1, 0, 2, 3))\n",
    "y_data = y_data.transpose((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve most recent 10% of data for testing\n",
    "train_index = int(X_data.shape[0] * 0.9)\n",
    "X_train = X_data[:train_index, :, :, :]\n",
    "y_train = y_data[:train_index, :, :]\n",
    "X_test = X_data[train_index:, :, :, :]\n",
    "y_test = y_data[train_index:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "validation_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking if the GPU is being used properly.\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() and not CPU else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=128):\n",
    "        super(TorchLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(1, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell state (h_0, c_0) with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h_0 = torch.zeros(1, batch_size, self.hidden_size, dtype=torch.float).to(x.device)\n",
    "        c_0 = torch.zeros(1, batch_size, self.hidden_size, dtype=torch.float).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))  # Shape: (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Take the last time step output\n",
    "        final_output = out[:, -1, :]  # Shape: (batch, hidden_size)\n",
    "\n",
    "        # Fully connected layer to map hidden state to final output\n",
    "        return self.fc(final_output)  # Shape: (batch, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_models = [TorchLSTM(hidden_size=HIDDEN_SIZE).to(DEVICE) for _ in range(SWARM_SIZE)]\n",
    "torch_optimizers = [torch.optim.Adam(model.parameters(), lr=LR) for model in torch_models]\n",
    "torch_loss = torch.nn.MSELoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_train(dataloader, models, optimizers):\n",
    "    losses = np.empty((0, SWARM_SIZE))\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = torch.transpose(X, 1, 0).type(torch.float32).to(DEVICE)\n",
    "        y = torch.transpose(y, 1, 0).type(torch.float32).to(DEVICE)\n",
    "\n",
    "        # Train\n",
    "        losses = np.vstack((losses, np.zeros(SWARM_SIZE)))\n",
    "        for idx, (model, optimizer) in enumerate(zip(models, optimizers)):\n",
    "            y_pred = model(X[idx])\n",
    "            loss = torch_loss(y_pred, y[idx])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses[batch, idx] = loss.item()\n",
    "\n",
    "        # Logging\n",
    "        if batch % LOG_FREQUENCY == 0:\n",
    "            # Display the first 5 model's losses (or less if less than 5)\n",
    "            loss = losses[-1]\n",
    "            current = batch * X[0].shape[0]\n",
    "            loss_string = \", \".join([f\"{loss[i]:.4f}\" for i in range(min(5, SWARM_SIZE))])\n",
    "            print(f\"loss: {loss_string}  [{current:>5d}]\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_validation(dataloader, models):\n",
    "    losses = np.empty((0, SWARM_SIZE))\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X = torch.transpose(X, 1, 0).type(torch.float32).to(DEVICE)\n",
    "            y = torch.transpose(y, 1, 0).type(torch.float32).to(DEVICE)\n",
    "\n",
    "            # Train\n",
    "            losses = np.vstack((losses, np.zeros(SWARM_SIZE)))\n",
    "            for idx, model in enumerate(models):\n",
    "                y_pred = model(X[idx])\n",
    "                loss = torch_loss(y_pred, y[idx])\n",
    "                losses[batch, idx] = loss.item()\n",
    "\n",
    "    # Logging\n",
    "    losses = losses.mean(axis=0)\n",
    "    loss_string = \", \".join([f\"{losses[i]:.4f}\" for i in range(min(5, SWARM_SIZE))])\n",
    "    print(f\"Validation loss: {loss_string}\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the PyTorch model\n",
    "start = time.time()\n",
    "train_epoch_no = []\n",
    "train_batch_loss = []\n",
    "valid_batch_loss = np.empty((0, SWARM_SIZE))\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {t + 1}\\n-------------------------------\")\n",
    "    # Training\n",
    "    for torch_model in torch_models:\n",
    "        torch_model.train()\n",
    "    _train_batch_losses = torch_train(train_dataloader, torch_models, torch_optimizers)\n",
    "\n",
    "    # Validation\n",
    "    for torch_model in torch_models:\n",
    "        torch_model.eval()\n",
    "    _valid_batch_losses = torch_validation(validation_dataloader, torch_models)\n",
    "\n",
    "    # Train data\n",
    "    for i in range(len(_train_batch_losses)):\n",
    "        train_epoch_no.append(t + float((i + 1) / len(_train_batch_losses)))\n",
    "        train_batch_loss.append(_train_batch_losses[i])\n",
    "\n",
    "    # Validation data\n",
    "    valid_batch_loss = np.vstack((valid_batch_loss, _valid_batch_losses))\n",
    "\n",
    "torch_time = time.time() - start\n",
    "print(f\"torch time: {torch_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_loss = np.array(train_batch_loss)\n",
    "valid_batch_loss = np.array(valid_batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of losses over training and validation\n",
    "sqrt_size = int(np.ceil(np.sqrt(SWARM_SIZE)))\n",
    "fig = plt.figure(figsize=(5 * sqrt_size, 5 * sqrt_size))\n",
    "for idx, ticker in enumerate(TICKERS):\n",
    "    fig.add_subplot(int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), idx + 1)\n",
    "    plt.plot(train_epoch_no, train_batch_loss[:, idx], label=ticker)\n",
    "    plt.plot(np.arange(1, EPOCHS + 1), valid_batch_loss[:, idx], label=f\"Validation {ticker}\")\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(TICKERS[idx] + \" Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_predictions = []\n",
    "for idx, torch_model in enumerate(torch_models):\n",
    "    torch_model.eval()\n",
    "    X = torch.Tensor(X_test.transpose((1, 0, 2, 3))[0]).to(DEVICE)\n",
    "    torch_predictions.append(torch_model(X).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.empty((0, SWARM_SIZE))\n",
    "torch_predictions = np.empty((0, SWARM_SIZE))\n",
    "for batch, (X, y) in enumerate(test_dataloader):\n",
    "    X = torch.transpose(X, 1, 0).type(torch.float32).to(DEVICE)\n",
    "    y = torch.transpose(y, 1, 0).type(torch.float32).to(DEVICE)\n",
    "\n",
    "    # Evaluate each model\n",
    "    y_true = np.vstack((y_true, y.reshape((SWARM_SIZE, X.shape[1])).transpose(1, 0).cpu()))\n",
    "    predictions = np.empty((X.shape[1], SWARM_SIZE))\n",
    "    for idx, model in enumerate(torch_models):\n",
    "        model.eval()\n",
    "\n",
    "        y_pred = model(X[idx])\n",
    "        predictions[:, idx] = y_pred.cpu().detach().numpy().T\n",
    "\n",
    "    torch_predictions = np.vstack((torch_predictions, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot of predictions vs ground truth (x by x)\n",
    "fig = plt.figure(figsize=(5 * sqrt_size, 5 * sqrt_size))\n",
    "\n",
    "# Shared axes\n",
    "for i in range(SWARM_SIZE):\n",
    "    ax = fig.add_subplot(\n",
    "        int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), i + 1\n",
    "    )\n",
    "    ax.plot(y_true[:, i], label=\"Ground Truth\")\n",
    "    ax.plot(torch_predictions[:, i], label=\"Torch Prediction\")\n",
    "    ax.set_title(TICKERS[i])\n",
    "    ax.set_ylabel(\"Price\")\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurbaLSTM(linen.Module):\n",
    "    features: int\n",
    "\n",
    "    @linen.compact\n",
    "    def __call__(self, x):\n",
    "        ScanLSTM = linen.scan(\n",
    "            linen.OptimizedLSTMCell,\n",
    "            variable_broadcast=\"params\",\n",
    "            split_rngs={\"params\": False},\n",
    "            in_axes=1,\n",
    "            out_axes=1,\n",
    "        )\n",
    "\n",
    "        lstm = ScanLSTM(self.features)\n",
    "        input_shape = x[:, 0].shape\n",
    "        carry = lstm.initialize_carry(jax.random.PRNGKey(0), input_shape)\n",
    "        carry, x = lstm(carry, x)\n",
    "        final = x[:, -1]\n",
    "        output = linen.Dense(1)(final)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turba_model = TurbaTrainState.swarm(\n",
    "    TurbaLSTM(features=HIDDEN_SIZE),\n",
    "    optimizer,\n",
    "    SWARM_SIZE,\n",
    "    X_data[0][0].reshape((1, TIME_WINDOW, 1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turba_train(dataloader: DataLoader, models: TurbaTrainState):\n",
    "    losses = np.empty((0, SWARM_SIZE))\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = torch.transpose(X, 1, 0)\n",
    "        y = torch.transpose(y, 1, 0)\n",
    "\n",
    "        # Train\n",
    "        models, loss, _ = models.train(X, y, mse)\n",
    "        losses = np.vstack((losses, loss))\n",
    "\n",
    "        # Logging\n",
    "        if batch % LOG_FREQUENCY == 0:\n",
    "            # Display the first 5 model's losses (or less if less than 5)\n",
    "            loss = losses[-1]\n",
    "            current = batch * X[0].shape[0]\n",
    "            loss_string = \", \".join([f\"{loss[i]:.4f}\" for i in range(min(5, SWARM_SIZE))])\n",
    "            print(f\"loss: {loss_string}  [{current:>5d}]\")\n",
    "\n",
    "    return models, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turba_validation(dataloader: DataLoader, models: TurbaTrainState):\n",
    "    losses = np.empty((0, SWARM_SIZE))\n",
    "    y_true = np.empty((0, SWARM_SIZE))\n",
    "    predictions = np.empty((0, SWARM_SIZE))\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = torch.transpose(X, 1, 0).numpy()\n",
    "        y = torch.transpose(y, 1, 0).numpy()\n",
    "\n",
    "        y_true = np.vstack((y_true, y.reshape((SWARM_SIZE, X.shape[1])).transpose(1, 0)))\n",
    "\n",
    "        # Train\n",
    "        loss, y_pred = models.evaluate(X, y, mse)\n",
    "        losses = np.vstack((losses, loss))\n",
    "        predictions = np.vstack(\n",
    "            (predictions, y_pred.reshape((SWARM_SIZE, X.shape[1])).transpose((1, 0)))\n",
    "        )\n",
    "\n",
    "    # Display the first 5 model's losses (or less if less than 5)\n",
    "    loss = losses[-1]\n",
    "    current = batch * X[0].shape[0]\n",
    "    loss_string = \", \".join([f\"{loss[i]:.4f}\" for i in range(min(5, SWARM_SIZE))])\n",
    "    print(f\"loss: {loss_string}  [{current:>5d}]\")\n",
    "\n",
    "    return losses.mean(axis=0), predictions, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Turba model\n",
    "start = time.time()\n",
    "train_epoch_no = []\n",
    "train_batch_loss = []\n",
    "valid_batch_loss = np.empty((0, SWARM_SIZE))\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {t + 1}\\n-------------------------------\")\n",
    "    # Training\n",
    "    turba_model, _train_batch_losses = turba_train(train_dataloader, turba_model)\n",
    "\n",
    "    # Validation\n",
    "    _valid_batch_losses, _, _ = turba_validation(validation_dataloader, turba_model)\n",
    "\n",
    "    # Train data\n",
    "    for i in range(len(_train_batch_losses)):\n",
    "        train_epoch_no.append(t + float((i + 1) / len(_train_batch_losses)))\n",
    "        train_batch_loss.append(_train_batch_losses[i])\n",
    "\n",
    "    # Validation data\n",
    "    valid_batch_loss = np.vstack((valid_batch_loss, _valid_batch_losses))\n",
    "\n",
    "turba_time = time.time() - start\n",
    "print(f\"turba time: {turba_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_loss = np.array(train_batch_loss)\n",
    "valid_batch_loss = np.array(valid_batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of losses over training and validation\n",
    "sqrt_size = int(np.ceil(np.sqrt(SWARM_SIZE)))\n",
    "fig = plt.figure(figsize=(5 * sqrt_size, 5 * sqrt_size))\n",
    "for idx, ticker in enumerate(TICKERS):\n",
    "    fig.add_subplot(int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), idx + 1)\n",
    "    plt.plot(train_epoch_no, train_batch_loss[:, idx], label=ticker)\n",
    "    plt.plot(np.arange(1, EPOCHS + 1), valid_batch_loss[:, idx], label=f\"Validation {ticker}\")\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(TICKERS[idx] + \" Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.empty((0, SWARM_SIZE))\n",
    "turba_predictions = np.empty((0, SWARM_SIZE))\n",
    "for batch, (X, y) in enumerate(test_dataloader):\n",
    "    X = torch.transpose(X, 1, 0).numpy()\n",
    "    y = torch.transpose(y, 1, 0).numpy()\n",
    "\n",
    "    y_true = np.vstack((y_true, y.reshape((SWARM_SIZE, X.shape[1])).transpose(1, 0)))\n",
    "\n",
    "    # Train\n",
    "    y_pred = turba_model.predict(X)\n",
    "    turba_predictions = np.vstack(\n",
    "        (turba_predictions, y_pred.reshape((SWARM_SIZE, X.shape[1])).transpose((1, 0)))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot of predictions vs ground truth (x by x)\n",
    "fig = plt.figure(figsize=(5 * sqrt_size, 5 * sqrt_size))\n",
    "\n",
    "# Shared axes\n",
    "for i in range(SWARM_SIZE):\n",
    "    ax = fig.add_subplot(\n",
    "        int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), i + 1\n",
    "    )\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(TICKERS[i])\n",
    "\n",
    "    # Axes\n",
    "    ax.set_ylabel(\"Price\")\n",
    "\n",
    "    # Data\n",
    "    ax.plot(y_true[:, i], label=\"Ground Truth\")\n",
    "    ax.plot(turba_predictions[:, i], label=\"Turba Prediction\")\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot of predictions vs ground truth (x by x)\n",
    "fig = plt.figure(figsize=(5 * sqrt_size, 5 * sqrt_size))\n",
    "\n",
    "# Shared axes\n",
    "for i in range(SWARM_SIZE):\n",
    "    ax = fig.add_subplot(\n",
    "        int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), i + 1\n",
    "    )\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(TICKERS[i])\n",
    "\n",
    "    # Axes\n",
    "    ax.set_ylabel(\"Price\")\n",
    "\n",
    "    # Data\n",
    "    ax.plot(y_true[:, i], label=\"Ground Truth\")\n",
    "    ax.plot(torch_predictions[:, i], label=\"Torch Prediction\")\n",
    "    ax.plot(turba_predictions[:, i], label=\"Turba Prediction\")\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display_functions, HTML\n",
    "\n",
    "output = \"\"\"\n",
    "===========================\n",
    "Model       Training Time\n",
    "===========================\n",
    "PyTorch       {:.2f} sec\n",
    "---------------------------\n",
    "Turba         {:.2f} sec\n",
    "===========================\n",
    "\"\"\".format(torch_time, turba_time)\n",
    "\n",
    "display_functions.display(HTML(f\"<pre>{output}</pre>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
