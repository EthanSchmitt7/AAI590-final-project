{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Value Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Instructions\n",
    "\n",
    "Before running this notebook, ensure you have the data ready. You have two options:\n",
    "\n",
    "1. **Automatic Download**: Run `setup.ipynb`, which will retrieve the data directly from github.\n",
    "2. **Manual Download**: Retrieve the dataset manually from [this Github repository](https://github.com/dhhagan/stocks/blob/master/scripts/stock_info.csv) and place the csv into the `data/input` folder.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook explores **training a neural network to predict stock prices** using **PyTorch** and **TurbaNet**, a framework for training multiple models concurrently. Our primary objectives include:\n",
    "\n",
    "- Training a **baseline PyTorch model** for each stock.\n",
    "- Training a **swarm of TurbaNet models** on the data.\n",
    "- Comparing the performance of PyTorch and TurbaNet models in terms of runtime.\n",
    "- Demonstrating the **effectivity of TurbaNet to train a large number of small models concurrently**, leveraging vectorization when memory constraints allow.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Training Approach\n",
    "\n",
    "#### **Control Model (PyTorch)**\n",
    "We start with a standard LSTM trained using PyTorch. \n",
    "- Each stock will be given its own independent model that PyTorch will train individually. \n",
    "- The models will be evaluated based on **loss metrics**.\n",
    "\n",
    "#### **Swarm-Based Training (TurbaNet)**\n",
    "Next, we employ TurbaNet to train Jax based LSTM models\n",
    "- Each stock will be given its own independent model that TurbaNet will train **simultaneously**. \n",
    "- This swarm-based approach aims to maximize **computational efficiency** while maintaining model performance.\n",
    "- The models will be evaluated based on **loss metrics**.\n",
    "\n",
    "#### **Performance Comparison**\n",
    "After training, we will:\n",
    "- Analyze batch-wise loss trends.\n",
    "- Compare the predictions from both approaches on all stocks.\n",
    "- Evaluate the real-time efficiency of TurbaNet compared to traditional training methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from flax import linen\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stockdex import Ticker\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from turbanet import TurbaTrainState, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Hyperparameters\n",
    "\n",
    "We define key hyperparameters for training our models, here is an example of a configuration:\n",
    "\n",
    "- **EPOCHS = 10,000** → Train for 10,000 full passes over the dataset.\n",
    "- **SWARM_SIZE = 100** → Train 100 models on different tickers concurrently using TurbaNet.\n",
    "- **BATCH_SIZE = 8** → Process 8 sequences per training batch.\n",
    "- **LR = 1e-5** → Learning rate set to `0.00001` for stable convergence.\n",
    "- **CPU = True** → Force PyTorch to use the CPU instead of the GPU. \n",
    "    - This can be helpful for comparing performance of the libraries when running on Windows machines as [Jax does not support GPUs running on Windows](https://docs.jax.dev/en/latest/installation.html).\n",
    "- **TIME_WINDOW = 20** → Sequence length for the input prior to the prediction.\n",
    "- **HIDDEN_SIZE = 32** → Size of the hidden layer of the network.\n",
    "\n",
    "These settings balance **training stability** and **efficiency**, leveraging **swarm-based training** to speed up model convergence.\n",
    "\n",
    "Feel free to tweak this values and examine the impact on training speed and effectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "SWARM_SIZE = 500\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-5\n",
    "CPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOW = 20\n",
    "HIDDEN_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "In this section, we have implemented the process of collecting, processing, and preparing the stock price data for use in training our model. Here’s what will happen:\n",
    "\n",
    "- **Stock Data Import**: We loaded the stock ticker information from a CSV file, which contains the tickers of various stocks.\n",
    "- **Sequence Creation**: We defined a function to generate sequences of stock price data, where each sequence is followed by the next price point, to use as input for time series modeling.\n",
    "- **Data Retrieval**: Stock data for each ticker is fetched from Yahoo Finance, normalized, and transformed into sequences.\n",
    "- **Data Filtering**: A random sample of tickers is selected, with validation to ensure that each ticker has sufficient data and consistent sequence lengths.\n",
    "- **Data Splitting**: The dataset is split into training, validation, and testing sets, ensuring that we reserve the most recent data for testing.\n",
    "- **PyTorch Data Preparation**: The data is prepared for use with PyTorch by converting it into tensors and creating data loaders for efficient batching during model training.\n",
    "\n",
    "Next, we will proceed to train models using the prepared data. This will involve building a time series model to predict stock prices based on the historical data we’ve collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in stock data\n",
    "df = pd.read_csv(\"../../data/input/stock_info.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences of price data of length TIME_WINDOW\n",
    "def create_sequences(data, time_window) -> tuple[np.ndarray, np.ndarray]:\n",
    "    sequences = []\n",
    "    results = []\n",
    "    for i in range(len(data) - time_window):\n",
    "        sequence = data[i : i + time_window]\n",
    "        sequences.append(sequence)\n",
    "        results.append(data[i + time_window])\n",
    "    return np.array(sequences).reshape(-1, time_window, 1), np.array(results).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker_data(ticker, date_range=\"1y\", data_granularity=\"1d\"):\n",
    "    data = Ticker(ticker=ticker).yahoo_api_price(\n",
    "        range=date_range, dataGranularity=data_granularity\n",
    "    )\n",
    "    close = data[\"close\"]\n",
    "    if close.empty:\n",
    "        return None\n",
    "    close /= np.max(close)\n",
    "    X_data, y_data = create_sequences(close, TIME_WINDOW)\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_ticker_data(stock_df, num_samples, date_range=\"1y\", data_granularity=\"1d\"):\n",
    "    tickers = []\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    max_seq_len = 0\n",
    "    while len(tickers) < num_samples:\n",
    "        ticker = stock_df.sample(1).Ticker.values[0]\n",
    "        try:\n",
    "            ticker_data = get_ticker_data(ticker, date_range, data_granularity)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # If the data returned is None, try again\n",
    "        if ticker_data is None:\n",
    "            continue\n",
    "\n",
    "        if ticker_data[0].shape[0] < max_seq_len:\n",
    "            continue\n",
    "\n",
    "        if ticker_data[0].shape[0] > max_seq_len:\n",
    "            max_seq_len = ticker_data[0].shape[0]\n",
    "            invalid_tickers = [x.shape[0] < max_seq_len for x in X_data]\n",
    "            if len(invalid_tickers) > 0:\n",
    "                del tickers[invalid_tickers]\n",
    "                del X_data[invalid_tickers]\n",
    "                del y_data[invalid_tickers]\n",
    "\n",
    "        tickers.append(ticker)\n",
    "        X_data.append(ticker_data[0])\n",
    "        y_data.append(ticker_data[1])\n",
    "\n",
    "        print(f\"Tickers Found: {len(tickers)}/{num_samples}\", end=\"\\r\", flush=True)\n",
    "\n",
    "    return tickers, np.array(X_data), np.array(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Data from Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKERS, X_data, y_data = get_random_ticker_data(\n",
    "    df, SWARM_SIZE, date_range=\"2y\", data_granularity=\"1d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Selected Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tickers_table(tickers, cols=5):\n",
    "    table = Table(title=\"Stock Tickers\", show_header=False)\n",
    "    for i in range(0, len(tickers), cols):\n",
    "        table.add_row(*tickers[i : i + cols], *[\"\"] * (cols - len(tickers[i : i + cols])))\n",
    "\n",
    "    console = Console()\n",
    "    console.print(table)\n",
    "\n",
    "\n",
    "display_tickers_table(TICKERS, cols=np.sqrt(len(TICKERS)).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "if BATCH_SIZE is None:\n",
    "    BATCH_SIZE = X_data.shape[1]\n",
    "X_data = X_data.transpose((1, 0, 2, 3))\n",
    "y_data = y_data.transpose((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve most recent 10% of data for testing\n",
    "train_index = int(X_data.shape[0] * 0.9)\n",
    "X_train = X_data[:train_index, :, :, :]\n",
    "y_train = y_data[:train_index, :, :]\n",
    "X_test = X_data[train_index:, :, :, :]\n",
    "y_test = y_data[train_index:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "validation_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PyTorch Model Training\n",
    "\n",
    "In this section, we create **baseline LSTMs** using PyTorch to establish a performance benchmark.\n",
    "\n",
    "### Training Details:\n",
    "- **Loss Function**: Cross-Entropy Loss\n",
    "- **Optimizer**: Adam\n",
    "\n",
    "This PyTorch model serves as a **control experiment**, helping us compare its efficiency and accuracy against the **swarm-based training** approach used in TurbaNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking if the GPU is being used properly.\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() and not CPU else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=128):\n",
    "        super(TorchLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(1, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell state (h_0, c_0) with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h_0 = torch.zeros(1, batch_size, self.hidden_size, dtype=torch.float).to(x.device)\n",
    "        c_0 = torch.zeros(1, batch_size, self.hidden_size, dtype=torch.float).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))  # Shape: (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Take the last time step output\n",
    "        final_output = out[:, -1, :]  # Shape: (batch, hidden_size)\n",
    "\n",
    "        # Fully connected layer to map hidden state to final output\n",
    "        return self.fc(final_output)  # Shape: (batch, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_models = [TorchLSTM(hidden_size=HIDDEN_SIZE).to(DEVICE) for _ in range(SWARM_SIZE)]\n",
    "torch_optimizers = [torch.optim.Adam(model.parameters(), lr=LR) for model in torch_models]\n",
    "torch_loss = torch.nn.MSELoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_train(dataloader, models, optimizers):\n",
    "    losses = np.empty((0, SWARM_SIZE))\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = torch.transpose(X, 1, 0).type(torch.float32).to(DEVICE)\n",
    "        y = torch.transpose(y, 1, 0).type(torch.float32).to(DEVICE)\n",
    "\n",
    "        # Train\n",
    "        losses = np.vstack((losses, np.zeros(SWARM_SIZE)))\n",
    "        for idx, (model, optimizer) in enumerate(zip(models, optimizers)):\n",
    "            y_pred = model(X[idx])\n",
    "            loss = torch_loss(y_pred, y[idx])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses[batch, idx] = loss.item()\n",
    "\n",
    "        # Logging\n",
    "        if batch % LOG_FREQUENCY == 0:\n",
    "            # Display the first 5 model's losses (or less if less than 5)\n",
    "            loss = losses[-1]\n",
    "            current = batch * X[0].shape[0]\n",
    "            loss_string = \", \".join([f\"{loss[i]:.4f}\" for i in range(min(5, SWARM_SIZE))])\n",
    "            print(f\"loss: {loss_string}  [{current:>5d}]\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_validation(dataloader, models):\n",
    "    losses = np.empty((0, SWARM_SIZE))\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X = torch.transpose(X, 1, 0).type(torch.float32).to(DEVICE)\n",
    "            y = torch.transpose(y, 1, 0).type(torch.float32).to(DEVICE)\n",
    "\n",
    "            # Train\n",
    "            losses = np.vstack((losses, np.zeros(SWARM_SIZE)))\n",
    "            for idx, model in enumerate(models):\n",
    "                y_pred = model(X[idx])\n",
    "                loss = torch_loss(y_pred, y[idx])\n",
    "                losses[batch, idx] = loss.item()\n",
    "\n",
    "    # Logging\n",
    "    losses = losses.mean(axis=0)\n",
    "    loss_string = \", \".join([f\"{losses[i]:.4f}\" for i in range(min(5, SWARM_SIZE))])\n",
    "    print(f\"Validation loss: {loss_string}\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the PyTorch model\n",
    "start = time.time()\n",
    "train_epoch_no = []\n",
    "train_batch_loss = []\n",
    "valid_batch_loss = np.empty((0, SWARM_SIZE))\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {t + 1}\\n-------------------------------\")\n",
    "    # Training\n",
    "    for torch_model in torch_models:\n",
    "        torch_model.train()\n",
    "    _train_batch_losses = torch_train(train_dataloader, torch_models, torch_optimizers)\n",
    "\n",
    "    # Validation\n",
    "    for torch_model in torch_models:\n",
    "        torch_model.eval()\n",
    "    _valid_batch_losses = torch_validation(validation_dataloader, torch_models)\n",
    "\n",
    "    # Train data\n",
    "    for i in range(len(_train_batch_losses)):\n",
    "        train_epoch_no.append(t + float((i + 1) / len(_train_batch_losses)))\n",
    "        train_batch_loss.append(_train_batch_losses[i])\n",
    "\n",
    "    # Validation data\n",
    "    valid_batch_loss = np.vstack((valid_batch_loss, _valid_batch_losses))\n",
    "\n",
    "torch_time = time.time() - start\n",
    "print(f\"torch time: {torch_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_loss = np.array(train_batch_loss)\n",
    "valid_batch_loss = np.array(valid_batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of losses over training and validation\n",
    "sqrt_size = int(np.ceil(np.sqrt(SWARM_SIZE)))\n",
    "fig = plt.figure(figsize=(5 * sqrt_size, 5 * sqrt_size))\n",
    "for idx, ticker in enumerate(TICKERS):\n",
    "    fig.add_subplot(int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), idx + 1)\n",
    "    plt.plot(train_epoch_no, train_batch_loss[:, idx], label=ticker)\n",
    "    plt.plot(np.arange(1, EPOCHS + 1), valid_batch_loss[:, idx], label=f\"Validation {ticker}\")\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(TICKERS[idx] + \" Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_predictions = []\n",
    "for idx, torch_model in enumerate(torch_models):\n",
    "    torch_model.eval()\n",
    "    X = torch.Tensor(X_test.transpose((1, 0, 2, 3))[0]).to(DEVICE)\n",
    "    torch_predictions.append(torch_model(X).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.empty((0, SWARM_SIZE))\n",
    "torch_predictions = np.empty((0, SWARM_SIZE))\n",
    "for batch, (X, y) in enumerate(test_dataloader):\n",
    "    X = torch.transpose(X, 1, 0).type(torch.float32).to(DEVICE)\n",
    "    y = torch.transpose(y, 1, 0).type(torch.float32).to(DEVICE)\n",
    "\n",
    "    # Evaluate each model\n",
    "    y_true = np.vstack((y_true, y.reshape((SWARM_SIZE, X.shape[1])).transpose(1, 0).cpu()))\n",
    "    predictions = np.empty((X.shape[1], SWARM_SIZE))\n",
    "    for idx, model in enumerate(torch_models):\n",
    "        model.eval()\n",
    "\n",
    "        y_pred = model(X[idx])\n",
    "        predictions[:, idx] = y_pred.cpu().detach().numpy().T\n",
    "\n",
    "    torch_predictions = np.vstack((torch_predictions, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot of predictions vs ground truth (x by x)\n",
    "fig = plt.figure(figsize=(5 * sqrt_size, 5 * sqrt_size))\n",
    "\n",
    "# Shared axes\n",
    "for i in range(SWARM_SIZE):\n",
    "    ax = fig.add_subplot(\n",
    "        int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), i + 1\n",
    "    )\n",
    "    ax.plot(y_true[:, i], label=\"Ground Truth\")\n",
    "    ax.plot(torch_predictions[:, i], label=\"Torch Prediction\")\n",
    "    ax.set_title(TICKERS[i])\n",
    "    ax.set_ylabel(\"Price\")\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurbaLSTM(linen.Module):\n",
    "    features: int\n",
    "\n",
    "    @linen.compact\n",
    "    def __call__(self, x):\n",
    "        ScanLSTM = linen.scan(\n",
    "            linen.OptimizedLSTMCell,\n",
    "            variable_broadcast=\"params\",\n",
    "            split_rngs={\"params\": False},\n",
    "            in_axes=1,\n",
    "            out_axes=1,\n",
    "        )\n",
    "\n",
    "        lstm = ScanLSTM(self.features)\n",
    "        input_shape = x[:, 0].shape\n",
    "        carry = lstm.initialize_carry(jax.random.PRNGKey(0), input_shape)\n",
    "        carry, x = lstm(carry, x)\n",
    "        final = x[:, -1]\n",
    "        output = linen.Dense(1)(final)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turba_model = TurbaTrainState.swarm(\n",
    "    TurbaLSTM(features=HIDDEN_SIZE),\n",
    "    optimizer,\n",
    "    SWARM_SIZE,\n",
    "    X_data[0][0].reshape((1, TIME_WINDOW, 1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turba_train(dataloader: DataLoader, models: TurbaTrainState):\n",
    "    losses = np.empty((0, SWARM_SIZE))\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = torch.transpose(X, 1, 0)\n",
    "        y = torch.transpose(y, 1, 0)\n",
    "\n",
    "        # Train\n",
    "        models, loss, _ = models.train(X, y, mse)\n",
    "        losses = np.vstack((losses, loss))\n",
    "\n",
    "        # Logging\n",
    "        if batch % LOG_FREQUENCY == 0:\n",
    "            # Display the first 5 model's losses (or less if less than 5)\n",
    "            loss = losses[-1]\n",
    "            current = batch * X[0].shape[0]\n",
    "            loss_string = \", \".join([f\"{loss[i]:.4f}\" for i in range(min(5, SWARM_SIZE))])\n",
    "            print(f\"loss: {loss_string}  [{current:>5d}]\")\n",
    "\n",
    "    return models, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turba_validation(dataloader: DataLoader, models: TurbaTrainState):\n",
    "    losses = np.empty((0, SWARM_SIZE))\n",
    "    y_true = np.empty((0, SWARM_SIZE))\n",
    "    predictions = np.empty((0, SWARM_SIZE))\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = torch.transpose(X, 1, 0).numpy()\n",
    "        y = torch.transpose(y, 1, 0).numpy()\n",
    "\n",
    "        y_true = np.vstack((y_true, y.reshape((SWARM_SIZE, X.shape[1])).transpose(1, 0)))\n",
    "\n",
    "        # Train\n",
    "        loss, y_pred = models.evaluate(X, y, mse)\n",
    "        losses = np.vstack((losses, loss))\n",
    "        predictions = np.vstack(\n",
    "            (predictions, y_pred.reshape((SWARM_SIZE, X.shape[1])).transpose((1, 0)))\n",
    "        )\n",
    "\n",
    "    # Display the first 5 model's losses (or less if less than 5)\n",
    "    loss = losses[-1]\n",
    "    current = batch * X[0].shape[0]\n",
    "    loss_string = \", \".join([f\"{loss[i]:.4f}\" for i in range(min(5, SWARM_SIZE))])\n",
    "    print(f\"loss: {loss_string}  [{current:>5d}]\")\n",
    "\n",
    "    return losses.mean(axis=0), predictions, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Turba model\n",
    "start = time.time()\n",
    "train_epoch_no = []\n",
    "train_batch_loss = []\n",
    "valid_batch_loss = np.empty((0, SWARM_SIZE))\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {t + 1}\\n-------------------------------\")\n",
    "    # Training\n",
    "    turba_model, _train_batch_losses = turba_train(train_dataloader, turba_model)\n",
    "\n",
    "    # Validation\n",
    "    _valid_batch_losses, _, _ = turba_validation(validation_dataloader, turba_model)\n",
    "\n",
    "    # Train data\n",
    "    for i in range(len(_train_batch_losses)):\n",
    "        train_epoch_no.append(t + float((i + 1) / len(_train_batch_losses)))\n",
    "        train_batch_loss.append(_train_batch_losses[i])\n",
    "\n",
    "    # Validation data\n",
    "    valid_batch_loss = np.vstack((valid_batch_loss, _valid_batch_losses))\n",
    "\n",
    "turba_time = time.time() - start\n",
    "print(f\"turba time: {turba_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_loss = np.array(train_batch_loss)\n",
    "valid_batch_loss = np.array(valid_batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of losses over training and validation\n",
    "sqrt_size = int(np.ceil(np.sqrt(SWARM_SIZE)))\n",
    "fig = plt.figure(figsize=(5 * sqrt_size, 5 * sqrt_size))\n",
    "for idx, ticker in enumerate(TICKERS):\n",
    "    fig.add_subplot(int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), idx + 1)\n",
    "    plt.plot(train_epoch_no, train_batch_loss[:, idx], label=ticker)\n",
    "    plt.plot(np.arange(1, EPOCHS + 1), valid_batch_loss[:, idx], label=f\"Validation {ticker}\")\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(TICKERS[idx] + \" Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.empty((0, SWARM_SIZE))\n",
    "turba_predictions = np.empty((0, SWARM_SIZE))\n",
    "for batch, (X, y) in enumerate(test_dataloader):\n",
    "    X = torch.transpose(X, 1, 0).numpy()\n",
    "    y = torch.transpose(y, 1, 0).numpy()\n",
    "\n",
    "    y_true = np.vstack((y_true, y.reshape((SWARM_SIZE, X.shape[1])).transpose(1, 0)))\n",
    "\n",
    "    # Train\n",
    "    y_pred = turba_model.predict(X)\n",
    "    turba_predictions = np.vstack(\n",
    "        (turba_predictions, y_pred.reshape((SWARM_SIZE, X.shape[1])).transpose((1, 0)))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot of predictions vs ground truth (x by x)\n",
    "fig = plt.figure(figsize=(5 * sqrt_size, 5 * sqrt_size))\n",
    "\n",
    "# Shared axes\n",
    "for i in range(SWARM_SIZE):\n",
    "    ax = fig.add_subplot(\n",
    "        int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), i + 1\n",
    "    )\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(TICKERS[i])\n",
    "\n",
    "    # Axes\n",
    "    ax.set_ylabel(\"Price\")\n",
    "\n",
    "    # Data\n",
    "    ax.plot(y_true[:, i], label=\"Ground Truth\")\n",
    "    ax.plot(turba_predictions[:, i], label=\"Turba Prediction\")\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot of predictions vs ground truth (x by x)\n",
    "fig = plt.figure(figsize=(5 * sqrt_size, 5 * sqrt_size))\n",
    "\n",
    "# Shared axes\n",
    "for i in range(SWARM_SIZE):\n",
    "    ax = fig.add_subplot(\n",
    "        int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), i + 1\n",
    "    )\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(TICKERS[i])\n",
    "\n",
    "    # Axes\n",
    "    ax.set_ylabel(\"Price\")\n",
    "\n",
    "    # Data\n",
    "    ax.plot(y_true[:, i], label=\"Ground Truth\")\n",
    "    ax.plot(torch_predictions[:, i], label=\"Torch Prediction\")\n",
    "    ax.plot(turba_predictions[:, i], label=\"Turba Prediction\")\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average error for each model\n",
    "error_torch = np.mean(np.abs(y_true - torch_predictions), axis=0)\n",
    "error_turba = np.mean(np.abs(y_true - turba_predictions), axis=0)\n",
    "error_torch = error_torch[~np.isnan(error_torch)]\n",
    "error_turba = error_turba[~np.isnan(error_turba)]\n",
    "\n",
    "# Combine both error arrays to calculate common bin edges\n",
    "combined_errors = np.concatenate([error_torch, error_turba])\n",
    "\n",
    "# Define the number of bins you want (e.g., 30 bins)\n",
    "num_bins = 30\n",
    "\n",
    "# Calculate bin edges based on combined data\n",
    "bin_edges = np.linspace(np.min(combined_errors), np.max(combined_errors), num_bins + 1)\n",
    "\n",
    "# Set the seaborn style for a polished look\n",
    "sns.set_theme(context=\"notebook\", style=\"whitegrid\", palette=\"tab10\")\n",
    "\n",
    "# Plotting the KDE with fill\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot histogram for Torch model using the common bin edges\n",
    "sns.histplot(\n",
    "    error_torch,\n",
    "    kde=True,\n",
    "    stat=\"probability\",\n",
    "    fill=True,\n",
    "    alpha=0.6,\n",
    "    linewidth=2,\n",
    "    bins=bin_edges,  # Use the predefined bin edges\n",
    "    label=\"Torch Models\",\n",
    ")\n",
    "\n",
    "# Plot histogram for Turba model using the common bin edges\n",
    "sns.histplot(\n",
    "    error_turba,\n",
    "    kde=True,\n",
    "    stat=\"probability\",\n",
    "    fill=True,\n",
    "    alpha=0.6,\n",
    "    linewidth=2,\n",
    "    bins=bin_edges,  # Use the predefined bin edges\n",
    "    label=\"Turba Models\",\n",
    ")\n",
    "\n",
    "# Add a title with a larger, bold font\n",
    "plt.title(\"Comparison of Average Errors: Torch vs. Turba\", fontsize=18, weight=\"bold\")\n",
    "\n",
    "# Add labels with larger, bolder fonts\n",
    "plt.xlabel(\"Average Error\", fontsize=15, weight=\"bold\")\n",
    "plt.ylabel(\"Probability\", fontsize=15, weight=\"bold\")\n",
    "\n",
    "# Increase the font size of the legend\n",
    "plt.legend(fontsize=14, title=\"Models\", title_fontsize=\"13\", loc=\"upper right\")\n",
    "\n",
    "# Refine the axes with ticks\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Remove the top and right borders for a cleaner look\n",
    "sns.despine()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display_functions, HTML\n",
    "\n",
    "output = \"\"\"\n",
    "===========================\n",
    "Model       Training Time\n",
    "===========================\n",
    "PyTorch       {:.2f} sec\n",
    "---------------------------\n",
    "Turba         {:.2f} sec\n",
    "===========================\n",
    "\"\"\".format(torch_time, turba_time)\n",
    "\n",
    "display_functions.display(HTML(f\"<pre>{output}</pre>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
