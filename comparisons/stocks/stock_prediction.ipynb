{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Value Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "from flax import linen\n",
    "from stockdex import Ticker\n",
    "from turbanet import TurbaTrainState, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOW = 20\n",
    "HIDDEN_SIZE = 32\n",
    "EPOCHS = 10_000\n",
    "LR = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKERS = [\"AAPL\", \"MSFT\", \"TSLA\", \"GOOGL\", \"AMZN\", \"NVDA\", \"META\", \"XOM\", \"UNH\", \"JNJ\"]\n",
    "SWARM_SIZE = len(TICKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences of price data of length TIME_WINDOW\n",
    "def create_sequences(data, time_window) -> tuple[np.ndarray, np.ndarray]:\n",
    "    sequences = []\n",
    "    results = []\n",
    "    for i in range(len(data) - time_window):\n",
    "        sequence = data[i : i + time_window]\n",
    "        sequences.append(sequence)\n",
    "        results.append(data[i + time_window])\n",
    "    return np.array(sequences).reshape(-1, time_window, 1), np.array(results).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker_data(ticker, range=\"1y\", dataGranularity=\"1d\"):\n",
    "    data = Ticker(ticker=ticker).yahoo_api_price(range=range, dataGranularity=dataGranularity)\n",
    "    close = data[\"close\"]\n",
    "    close /= np.max(close)\n",
    "    X_data, y_data = create_sequences(close, TIME_WINDOW)\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_data = [get_ticker_data(ticker) for ticker in TICKERS]\n",
    "X_data = np.array([data[0] for data in ticker_data])\n",
    "y_data = np.array([data[1] for data in ticker_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurbaLSTM(linen.Module):\n",
    "    features: int\n",
    "\n",
    "    @linen.compact\n",
    "    def __call__(self, x):\n",
    "        ScanLSTM = linen.scan(\n",
    "            linen.OptimizedLSTMCell,\n",
    "            variable_broadcast=\"params\",\n",
    "            split_rngs={\"params\": False},\n",
    "            in_axes=1,\n",
    "            out_axes=1,\n",
    "        )\n",
    "\n",
    "        lstm = ScanLSTM(self.features)\n",
    "        input_shape = x[:, 0].shape\n",
    "        carry = lstm.initialize_carry(jax.random.PRNGKey(0), input_shape)\n",
    "        carry, x = lstm(carry, x)\n",
    "        final = x[:, -1]\n",
    "        output = linen.Dense(1)(final)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turba_model = TurbaTrainState.swarm(TurbaLSTM(features=HIDDEN_SIZE), optimizer, SWARM_SIZE, X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Turba model\n",
    "start = time.time()\n",
    "turba_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    turba_model, loss, pred = turba_model.train(X_data, y_data, mse)\n",
    "\n",
    "    # Logging\n",
    "    turba_losses.append(loss)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch} Losses: {jnp.array(turba_losses[-100:]).mean(axis=0)}\")\n",
    "\n",
    "print(f\"Turba time: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turba_losses = jnp.array(turba_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turba_predictions = turba_model.predict(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of losses over training\n",
    "plt.figure(figsize=(32, 16))\n",
    "plt.plot(turba_losses, label=TICKERS)\n",
    "plt.title(\"Loss\", fontsize=32)\n",
    "plt.xlabel(\"Epochs\", fontsize=24)\n",
    "plt.ylabel(\"Loss\", fontsize=24)\n",
    "plt.legend(fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot of predictions vs ground truth (x by x)\n",
    "fig = plt.figure(figsize=(32, 16))\n",
    "\n",
    "# Shared axes\n",
    "for i in range(SWARM_SIZE):\n",
    "    ax = fig.add_subplot(\n",
    "        int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), i + 1\n",
    "    )\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(TICKERS[i])\n",
    "\n",
    "    # Axes\n",
    "    ax.set_ylabel(\"Price\")\n",
    "\n",
    "    # Data\n",
    "    ax.plot(y_data[i], label=\"Ground Truth\")\n",
    "    ax.plot(turba_predictions[i], label=\"Turba Prediction\")\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TorchLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=128):\n",
    "        super(TorchLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(1, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell state (h_0, c_0) with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h_0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))  # Shape: (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Take the last time step output\n",
    "        final_output = out[:, -1, :]  # Shape: (batch, hidden_size)\n",
    "\n",
    "        # Fully connected layer to map hidden state to final output\n",
    "        return self.fc(final_output)  # Shape: (batch, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_models = [TorchLSTM(hidden_size=HIDDEN_SIZE) for _ in range(SWARM_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the PyTorch model\n",
    "start = time.time()\n",
    "\n",
    "torch_loss = torch.nn.MSELoss()\n",
    "torch_losses = []\n",
    "for idx, torch_model in enumerate(torch_models):\n",
    "    torch_model.train()\n",
    "    torch_optimizer = torch.optim.Adam(torch_model.parameters(), lr=LR)\n",
    "\n",
    "    torch_losses.append([])\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Train\n",
    "        torch_optimizer.zero_grad()\n",
    "        y_pred = torch_model(torch.Tensor(X_data[idx]))\n",
    "        loss = torch_loss(y_pred, torch.Tensor(y_data[idx]))\n",
    "        loss.backward()\n",
    "        torch_optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        torch_losses[idx].append(loss.item())\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Model {idx} - Epoch {epoch} Loss: {np.mean(torch_losses[idx][-100:])}\")\n",
    "\n",
    "print(f\"torch time: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_predictions = []\n",
    "for idx, torch_model in enumerate(torch_models):\n",
    "    torch_model.eval()\n",
    "    torch_predictions.append(torch_model(torch.Tensor(X_data[idx])).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_losses = np.array(torch_losses).T\n",
    "torch_predictions = np.array(torch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of losses over training\n",
    "plt.figure(figsize=(32, 16))\n",
    "plt.plot(torch_losses, label=TICKERS)\n",
    "plt.title(\"Loss\", fontsize=32)\n",
    "plt.xlabel(\"Epochs\", fontsize=24)\n",
    "plt.ylabel(\"Loss\", fontsize=24)\n",
    "plt.legend(fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot of predictions vs ground truth (x by x)\n",
    "fig = plt.figure(figsize=(32, 16))\n",
    "\n",
    "# Shared axes\n",
    "for i in range(SWARM_SIZE):\n",
    "    ax = fig.add_subplot(\n",
    "        int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), i + 1\n",
    "    )\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(TICKERS[i])\n",
    "\n",
    "    # Axes\n",
    "    ax.set_ylabel(\"Price\")\n",
    "\n",
    "    # Data\n",
    "    ax.plot(y_data[i], label=\"Ground Truth\")\n",
    "    ax.plot(torch_predictions[i], label=\"Torch Prediction\")\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot of predictions vs ground truth (x by x)\n",
    "fig = plt.figure(figsize=(32, 20))\n",
    "\n",
    "# Shared axes\n",
    "for i in range(SWARM_SIZE):\n",
    "    ax = fig.add_subplot(\n",
    "        int(np.ceil(np.sqrt(SWARM_SIZE))), int(np.ceil(np.sqrt(SWARM_SIZE))), i + 1\n",
    "    )\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(TICKERS[i])\n",
    "\n",
    "    # Axes\n",
    "    ax.set_ylabel(\"Price\")\n",
    "\n",
    "    # Data\n",
    "    ax.plot(y_data[i], label=\"Ground Truth\")\n",
    "    ax.plot(torch_predictions[i], label=\"Torch Prediction\")\n",
    "    ax.plot(turba_predictions[i], label=\"Turba Prediction\")\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
