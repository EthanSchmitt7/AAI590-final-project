{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from flax import linen\n",
    "from jax.lib import xla_bridge\n",
    "from turbanet import TurbaTrainState\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL INPUTS\n",
    "GPU = False\n",
    "repeats = 10\n",
    "\n",
    "# NETWORK SHAPE INPUTS\n",
    "hidden_sizes = [16]  # 8 * np.arange(1, 65)\n",
    "num_layers = [1]\n",
    "\n",
    "# TRAINING INPUTS\n",
    "lr = 1e-3\n",
    "dataset_size = 128\n",
    "swarm_sizes = 2 * np.arange(1, 65)\n",
    "epochs = [1024]\n",
    "batch_sizes = [128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinations: 64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Combinations: {len(hidden_sizes) * len(num_layers) * len(swarm_sizes) * len(epochs) * len(batch_sizes)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21c5becd8b0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set numpy/torch/flax seeds\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random data\n",
    "def make_spirals(n_samples, noise_std=0.0, rotations=1.0):\n",
    "    ts = jnp.linspace(0, 1, n_samples)\n",
    "    rs = ts**0.5\n",
    "    thetas = rs * rotations * 2 * np.pi\n",
    "    signs = np.random.randint(0, 2, (n_samples,)) * 2 - 1\n",
    "    labels = (signs > 0).astype(int)\n",
    "\n",
    "    xs = rs * signs * jnp.cos(thetas) + np.random.randn(n_samples) * noise_std\n",
    "    ys = rs * signs * jnp.sin(thetas) + np.random.randn(n_samples) * noise_std\n",
    "    points = jnp.stack([xs, ys], axis=1)\n",
    "    return points, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, labels = make_spirals(dataset_size, noise_std=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchModel(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_layers: int):\n",
    "        super(TorchModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            *(nn.Linear(hidden_size, hidden_size), nn.ReLU()) * (num_layers - 1),\n",
    "            nn.Linear(hidden_size, 2),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_train(torch_models, torch_optimizers, epochs, X, y):\n",
    "    for torch_model, torch_optimizer in zip(torch_models, torch_optimizers):\n",
    "        torch_model.train()\n",
    "        for _ in range(epochs):\n",
    "            for batch_input, batch_label in zip(X, y):\n",
    "                torch_optimizer.zero_grad()\n",
    "                y_pred = torch_model(batch_input)\n",
    "                loss = torch.nn.functional.cross_entropy(y_pred, batch_label)\n",
    "                loss.backward()\n",
    "                torch_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = []\n",
    "layers = []\n",
    "swarm = []\n",
    "epoch = []\n",
    "batch = []\n",
    "times = []\n",
    "losses = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    for num_layer in num_layers:\n",
    "        for swarm_size in swarm_sizes:\n",
    "            for epoch_num in epochs:\n",
    "                for batch_size in batch_sizes:\n",
    "                    # Create models\n",
    "                    torch_models = [TorchModel(hidden_size, num_layer) for _ in range(swarm_size)]\n",
    "\n",
    "                    # Create optimizers\n",
    "                    torch_optimizers = [\n",
    "                        torch.optim.Adam(torch_model.parameters(), lr=lr)\n",
    "                        for torch_model in torch_models\n",
    "                    ]\n",
    "\n",
    "                    # Set torch to use GPU if available\n",
    "                    device = torch.device(\"cpu\")\n",
    "                    if GPU and torch.cuda.is_available():\n",
    "                        device = torch.device(\"cuda\")\n",
    "                        for torch_model in torch_models:\n",
    "                            torch_model.to(device)\n",
    "\n",
    "                    # Prepare data\n",
    "                    X_train_torch = torch.from_numpy(\n",
    "                        np.array(points.reshape(-1, batch_size, 2))\n",
    "                    ).float()\n",
    "                    y_train_torch = torch.from_numpy(np.array(labels.reshape(-1, batch_size)))\n",
    "\n",
    "                    # Move to GPU if available\n",
    "                    if GPU:\n",
    "                        X_train_torch = X_train_torch.to(device)\n",
    "                        y_train_torch = y_train_torch.to(device)\n",
    "\n",
    "                    # Train\n",
    "                    train_times = np.zeros(repeats)\n",
    "                    for r in range(repeats):\n",
    "                        start = time()\n",
    "                        torch_train(\n",
    "                            torch_models, torch_optimizers, epoch_num, X_train_torch, y_train_torch\n",
    "                        )\n",
    "                        train_times[r] = time() - start\n",
    "\n",
    "                    train_time = train_times.mean()\n",
    "\n",
    "                    # Print results\n",
    "                    print(\n",
    "                        f\"Hidden Nodes: {hidden_size}, \"\n",
    "                        f\"Layers: {num_layer}, \"\n",
    "                        f\"Swarm: {swarm_size}, \"\n",
    "                        f\"Epochs: {epoch_num}, \"\n",
    "                        f\"Batch: {batch_size}, \"\n",
    "                        f\"Time: {train_time}, \"\n",
    "                    )\n",
    "\n",
    "                    # Save results\n",
    "                    hidden.append(hidden_size)\n",
    "                    layers.append(num_layer)\n",
    "                    swarm.append(swarm_size)\n",
    "                    epoch.append(epoch_num)\n",
    "                    batch.append(batch_size)\n",
    "                    times.append(train_time)\n",
    "\n",
    "# Output results as dataframe\n",
    "torch_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Hidden\": hidden,\n",
    "        \"Layers\": layers,\n",
    "        \"Swarm\": swarm,\n",
    "        \"Epoch\": epoch,\n",
    "        \"Batch\": batch,\n",
    "        \"Time\": times,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save timing data\n",
    "torch_data.to_csv(\"../../data/output/timing/torch_swarm_size_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_turba(params, input, output, apply_fn):\n",
    "    log_probs = apply_fn({\"params\": params}, input)\n",
    "    labels = jax.nn.one_hot(output, log_probs.shape[1])\n",
    "    loss = -jnp.mean(jnp.sum(labels * log_probs, axis=1))\n",
    "    return loss, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaxModel(linen.Module):\n",
    "    hidden_layers: int = 1\n",
    "    hidden_dim: int = 32\n",
    "\n",
    "    @linen.compact\n",
    "    def __call__(self, x):\n",
    "        for layer in range(self.hidden_layers):\n",
    "            x = linen.Dense(self.hidden_dim)(x)\n",
    "            x = linen.relu(x)\n",
    "        x = linen.Dense(2)(x)\n",
    "        x = linen.log_softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turba_train(turba_state, epochs, X, y):\n",
    "    for _ in range(epochs):\n",
    "        for batch_input, batch_label in zip(X, y):\n",
    "            turba_state, _, _ = turba_state.train(batch_input, batch_label, cross_entropy_turba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Nodes: 16, Layers: 1, Swarm: 2, Epochs: 1024, Batch: 128, Time: 0.6248784065246582, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 4, Epochs: 1024, Batch: 128, Time: 0.6310785293579102, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 6, Epochs: 1024, Batch: 128, Time: 0.6246379137039184, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 8, Epochs: 1024, Batch: 128, Time: 0.6039792537689209, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 10, Epochs: 1024, Batch: 128, Time: 0.6036675930023193, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 12, Epochs: 1024, Batch: 128, Time: 0.6016450881958008, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 14, Epochs: 1024, Batch: 128, Time: 0.6072637557983398, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 16, Epochs: 1024, Batch: 128, Time: 0.6071071147918701, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 18, Epochs: 1024, Batch: 128, Time: 0.6074105978012085, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 20, Epochs: 1024, Batch: 128, Time: 0.607907748222351, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 22, Epochs: 1024, Batch: 128, Time: 0.6154222965240479, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 24, Epochs: 1024, Batch: 128, Time: 0.6189663887023926, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 26, Epochs: 1024, Batch: 128, Time: 0.6619048357009888, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 28, Epochs: 1024, Batch: 128, Time: 0.6648490905761719, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 30, Epochs: 1024, Batch: 128, Time: 0.7087999582290649, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 32, Epochs: 1024, Batch: 128, Time: 0.7641155481338501, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 34, Epochs: 1024, Batch: 128, Time: 0.8040279388427735, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 36, Epochs: 1024, Batch: 128, Time: 0.8314825057983398, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 38, Epochs: 1024, Batch: 128, Time: 0.8680797576904297, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 40, Epochs: 1024, Batch: 128, Time: 0.9098431587219238, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 42, Epochs: 1024, Batch: 128, Time: 1.123918080329895, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 44, Epochs: 1024, Batch: 128, Time: 1.1610198974609376, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 46, Epochs: 1024, Batch: 128, Time: 1.2083261728286743, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 48, Epochs: 1024, Batch: 128, Time: 1.255153226852417, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 50, Epochs: 1024, Batch: 128, Time: 1.300092339515686, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 52, Epochs: 1024, Batch: 128, Time: 1.3184545755386352, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 54, Epochs: 1024, Batch: 128, Time: 1.3757574319839478, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 56, Epochs: 1024, Batch: 128, Time: 1.4098030090332032, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 58, Epochs: 1024, Batch: 128, Time: 1.449502182006836, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 60, Epochs: 1024, Batch: 128, Time: 1.4897345066070558, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 62, Epochs: 1024, Batch: 128, Time: 1.5352469682693481, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 64, Epochs: 1024, Batch: 128, Time: 1.5749717235565186, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 66, Epochs: 1024, Batch: 128, Time: 1.6471479654312133, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 68, Epochs: 1024, Batch: 128, Time: 1.6809578657150268, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 70, Epochs: 1024, Batch: 128, Time: 1.7298282623291015, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 72, Epochs: 1024, Batch: 128, Time: 1.7459636449813842, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 74, Epochs: 1024, Batch: 128, Time: 1.8068448066711427, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 76, Epochs: 1024, Batch: 128, Time: 1.8179227352142333, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 78, Epochs: 1024, Batch: 128, Time: 1.8598203182220459, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 80, Epochs: 1024, Batch: 128, Time: 1.8986180067062377, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 82, Epochs: 1024, Batch: 128, Time: 1.953029727935791, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 84, Epochs: 1024, Batch: 128, Time: 2.0174165964126587, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 86, Epochs: 1024, Batch: 128, Time: 2.067028260231018, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 88, Epochs: 1024, Batch: 128, Time: 2.1100820779800413, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 90, Epochs: 1024, Batch: 128, Time: 2.157739591598511, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 92, Epochs: 1024, Batch: 128, Time: 2.2455076694488527, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 94, Epochs: 1024, Batch: 128, Time: 2.287831258773804, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 96, Epochs: 1024, Batch: 128, Time: 2.3307938814163207, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 98, Epochs: 1024, Batch: 128, Time: 2.60079140663147, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 100, Epochs: 1024, Batch: 128, Time: 2.6089908361434935, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 102, Epochs: 1024, Batch: 128, Time: 2.468178868293762, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 104, Epochs: 1024, Batch: 128, Time: 3.509673595428467, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 106, Epochs: 1024, Batch: 128, Time: 2.5774096250534058, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 108, Epochs: 1024, Batch: 128, Time: 2.8485528230667114, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 110, Epochs: 1024, Batch: 128, Time: 3.3452961444854736, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 112, Epochs: 1024, Batch: 128, Time: 3.61488881111145, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 114, Epochs: 1024, Batch: 128, Time: 3.670903944969177, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 116, Epochs: 1024, Batch: 128, Time: 3.398326802253723, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 118, Epochs: 1024, Batch: 128, Time: 3.012543296813965, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 120, Epochs: 1024, Batch: 128, Time: 3.0880661010742188, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 122, Epochs: 1024, Batch: 128, Time: 3.4444231033325194, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 124, Epochs: 1024, Batch: 128, Time: 3.5739388704299926, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 126, Epochs: 1024, Batch: 128, Time: 3.679357481002808, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 128, Epochs: 1024, Batch: 128, Time: 3.6327510833740235, \n"
     ]
    }
   ],
   "source": [
    "hidden = []\n",
    "layers = []\n",
    "swarm = []\n",
    "epoch = []\n",
    "batch = []\n",
    "times = []\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for num_layer in num_layers:\n",
    "        for swarm_size in swarm_sizes:\n",
    "            for epoch_num in epochs:\n",
    "                for batch_size in batch_sizes:\n",
    "                    # Create models\n",
    "                    turba_model = JaxModel(hidden_layers=num_layer, hidden_dim=hidden_size)\n",
    "                    turba_state = TurbaTrainState.swarm(\n",
    "                        turba_model, swarm_size, 2, learning_rate=lr\n",
    "                    )\n",
    "\n",
    "                    # Set Turba to use GPU if available\n",
    "                    if GPU and xla_bridge.get_backend().platform != \"gpu\":\n",
    "                        raise RuntimeError(\"GPU support not available for Turba.\")\n",
    "\n",
    "                    # Prepare data\n",
    "                    X_train_turba = jnp.array(\n",
    "                        np.expand_dims(points.reshape(-1, batch_size, 2), axis=1).repeat(\n",
    "                            swarm_size, axis=1\n",
    "                        ),\n",
    "                        dtype=jnp.float32,\n",
    "                    )\n",
    "                    y_train_turba = jnp.array(\n",
    "                        np.expand_dims(labels.reshape(-1, batch_size), axis=1).repeat(\n",
    "                            swarm_size, axis=1\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # Train\n",
    "                    train_times = np.zeros(repeats)\n",
    "                    for r in range(repeats):\n",
    "                        start = time()\n",
    "                        turba_train(turba_state, epoch_num, X_train_turba, y_train_turba)\n",
    "                        train_times[r] = time() - start\n",
    "\n",
    "                    train_time = train_times.mean()\n",
    "\n",
    "                    # Print results\n",
    "                    print(\n",
    "                        f\"Hidden Nodes: {hidden_size}, \"\n",
    "                        f\"Layers: {num_layer}, \"\n",
    "                        f\"Swarm: {swarm_size}, \"\n",
    "                        f\"Epochs: {epoch_num}, \"\n",
    "                        f\"Batch: {batch_size}, \"\n",
    "                        f\"Time: {train_time}, \"\n",
    "                    )\n",
    "\n",
    "                    # Save results\n",
    "                    hidden.append(hidden_size)\n",
    "                    layers.append(num_layer)\n",
    "                    swarm.append(swarm_size)\n",
    "                    epoch.append(epoch_num)\n",
    "                    batch.append(batch_size)\n",
    "                    times.append(train_time)\n",
    "\n",
    "# Output results as dataframe\n",
    "turba_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Hidden\": hidden,\n",
    "        \"Layers\": layers,\n",
    "        \"Swarm\": swarm,\n",
    "        \"Epoch\": epoch,\n",
    "        \"Batch\": batch,\n",
    "        \"Time\": times,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save timing data\n",
    "turba_data.to_csv(\"../../data/output/timing/turba_swarm_size_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
