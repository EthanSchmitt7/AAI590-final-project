{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from flax import linen\n",
    "from jax.lib import xla_bridge\n",
    "from turbanet import TurbaTrainState\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL INPUTS\n",
    "GPU = False\n",
    "repeats = 10\n",
    "\n",
    "# NETWORK SHAPE INPUTS\n",
    "hidden_sizes = [16]  # 8 * np.arange(1, 65)\n",
    "num_layers = [1]\n",
    "\n",
    "# TRAINING INPUTS\n",
    "lr = 1e-3\n",
    "dataset_size = 128\n",
    "swarm_sizes = 2 * np.arange(1, 65)\n",
    "epochs = [1024]\n",
    "batch_sizes = [128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Combinations: {len(hidden_sizes) * len(num_layers) * len(swarm_sizes) * len(epochs) * len(batch_sizes)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set numpy/torch/flax seeds\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random data\n",
    "def make_spirals(n_samples, noise_std=0.0, rotations=1.0):\n",
    "    ts = jnp.linspace(0, 1, n_samples)\n",
    "    rs = ts**0.5\n",
    "    thetas = rs * rotations * 2 * np.pi\n",
    "    signs = np.random.randint(0, 2, (n_samples,)) * 2 - 1\n",
    "    labels = (signs > 0).astype(int)\n",
    "\n",
    "    xs = rs * signs * jnp.cos(thetas) + np.random.randn(n_samples) * noise_std\n",
    "    ys = rs * signs * jnp.sin(thetas) + np.random.randn(n_samples) * noise_std\n",
    "    points = jnp.stack([xs, ys], axis=1)\n",
    "    return points, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, labels = make_spirals(dataset_size, noise_std=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchModel(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_layers: int):\n",
    "        super(TorchModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            *(nn.Linear(hidden_size, hidden_size), nn.ReLU()) * (num_layers - 1),\n",
    "            nn.Linear(hidden_size, 2),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_train(torch_models, torch_optimizers, epochs, X, y):\n",
    "    for torch_model, torch_optimizer in zip(torch_models, torch_optimizers):\n",
    "        torch_model.train()\n",
    "        for _ in range(epochs):\n",
    "            for batch_input, batch_label in zip(X, y):\n",
    "                torch_optimizer.zero_grad()\n",
    "                y_pred = torch_model(batch_input)\n",
    "                loss = torch.nn.functional.cross_entropy(y_pred, batch_label)\n",
    "                loss.backward()\n",
    "                torch_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = []\n",
    "layers = []\n",
    "swarm = []\n",
    "epoch = []\n",
    "batch = []\n",
    "times = []\n",
    "losses = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    for num_layer in num_layers:\n",
    "        for swarm_size in swarm_sizes:\n",
    "            for epoch_num in epochs:\n",
    "                for batch_size in batch_sizes:\n",
    "                    # Create models\n",
    "                    torch_models = [TorchModel(hidden_size, num_layer) for _ in range(swarm_size)]\n",
    "\n",
    "                    # Create optimizers\n",
    "                    torch_optimizers = [\n",
    "                        torch.optim.Adam(torch_model.parameters(), lr=lr)\n",
    "                        for torch_model in torch_models\n",
    "                    ]\n",
    "\n",
    "                    # Set torch to use GPU if available\n",
    "                    device = torch.device(\"cpu\")\n",
    "                    if GPU and torch.cuda.is_available():\n",
    "                        device = torch.device(\"cuda\")\n",
    "                        for torch_model in torch_models:\n",
    "                            torch_model.to(device)\n",
    "\n",
    "                    # Prepare data\n",
    "                    X_train_torch = torch.from_numpy(\n",
    "                        np.array(points.reshape(-1, batch_size, 2))\n",
    "                    ).float()\n",
    "                    y_train_torch = torch.from_numpy(np.array(labels.reshape(-1, batch_size)))\n",
    "\n",
    "                    # Move to GPU if available\n",
    "                    if GPU:\n",
    "                        X_train_torch = X_train_torch.to(device)\n",
    "                        y_train_torch = y_train_torch.to(device)\n",
    "\n",
    "                    # Train\n",
    "                    train_times = np.zeros(repeats)\n",
    "                    for r in range(repeats):\n",
    "                        start = time()\n",
    "                        torch_train(\n",
    "                            torch_models, torch_optimizers, epoch_num, X_train_torch, y_train_torch\n",
    "                        )\n",
    "                        train_times[r] = time() - start\n",
    "\n",
    "                    train_time = train_times.mean()\n",
    "\n",
    "                    # Print results\n",
    "                    print(\n",
    "                        f\"Hidden Nodes: {hidden_size}, \"\n",
    "                        f\"Layers: {num_layer}, \"\n",
    "                        f\"Swarm: {swarm_size}, \"\n",
    "                        f\"Epochs: {epoch_num}, \"\n",
    "                        f\"Batch: {batch_size}, \"\n",
    "                        f\"Time: {train_time}, \"\n",
    "                    )\n",
    "\n",
    "                    # Save results\n",
    "                    hidden.append(hidden_size)\n",
    "                    layers.append(num_layer)\n",
    "                    swarm.append(swarm_size)\n",
    "                    epoch.append(epoch_num)\n",
    "                    batch.append(batch_size)\n",
    "                    times.append(train_time)\n",
    "\n",
    "# Output results as dataframe\n",
    "torch_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Hidden\": hidden,\n",
    "        \"Layers\": layers,\n",
    "        \"Swarm\": swarm,\n",
    "        \"Epoch\": epoch,\n",
    "        \"Batch\": batch,\n",
    "        \"Time\": times,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save timing data\n",
    "torch_data.to_csv(\"../../data/output/timing/torch_swarm_size_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_turba(params, input, output, apply_fn):\n",
    "    log_probs = apply_fn({\"params\": params}, input)\n",
    "    labels = jax.nn.one_hot(output, log_probs.shape[1])\n",
    "    loss = -jnp.mean(jnp.sum(labels * log_probs, axis=1))\n",
    "    return loss, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaxModel(linen.Module):\n",
    "    hidden_layers: int = 1\n",
    "    hidden_dim: int = 32\n",
    "\n",
    "    @linen.compact\n",
    "    def __call__(self, x):\n",
    "        for layer in range(self.hidden_layers):\n",
    "            x = linen.Dense(self.hidden_dim)(x)\n",
    "            x = linen.relu(x)\n",
    "        x = linen.Dense(2)(x)\n",
    "        x = linen.log_softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turba_train(turba_state, epochs, X, y):\n",
    "    for _ in range(epochs):\n",
    "        for batch_input, batch_label in zip(X, y):\n",
    "            turba_state, _, _ = turba_state.train(batch_input, batch_label, cross_entropy_turba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = []\n",
    "layers = []\n",
    "swarm = []\n",
    "epoch = []\n",
    "batch = []\n",
    "times = []\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for num_layer in num_layers:\n",
    "        for swarm_size in swarm_sizes:\n",
    "            for epoch_num in epochs:\n",
    "                for batch_size in batch_sizes:\n",
    "                    # Create models\n",
    "                    turba_model = JaxModel(hidden_layers=num_layer, hidden_dim=hidden_size)\n",
    "                    turba_state = TurbaTrainState.swarm(\n",
    "                        turba_model, swarm_size, 2, learning_rate=lr\n",
    "                    )\n",
    "\n",
    "                    # Set Turba to use GPU if available\n",
    "                    if GPU and xla_bridge.get_backend().platform != \"gpu\":\n",
    "                        raise RuntimeError(\"GPU support not available for Turba.\")\n",
    "\n",
    "                    # Prepare data\n",
    "                    X_train_turba = jnp.array(\n",
    "                        np.expand_dims(points.reshape(-1, batch_size, 2), axis=1).repeat(\n",
    "                            swarm_size, axis=1\n",
    "                        ),\n",
    "                        dtype=jnp.float32,\n",
    "                    )\n",
    "                    y_train_turba = jnp.array(\n",
    "                        np.expand_dims(labels.reshape(-1, batch_size), axis=1).repeat(\n",
    "                            swarm_size, axis=1\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # Train\n",
    "                    train_times = np.zeros(repeats)\n",
    "                    for r in range(repeats):\n",
    "                        start = time()\n",
    "                        turba_train(turba_state, epoch_num, X_train_turba, y_train_turba)\n",
    "                        train_times[r] = time() - start\n",
    "\n",
    "                    train_time = train_times.mean()\n",
    "\n",
    "                    # Print results\n",
    "                    print(\n",
    "                        f\"Hidden Nodes: {hidden_size}, \"\n",
    "                        f\"Layers: {num_layer}, \"\n",
    "                        f\"Swarm: {swarm_size}, \"\n",
    "                        f\"Epochs: {epoch_num}, \"\n",
    "                        f\"Batch: {batch_size}, \"\n",
    "                        f\"Time: {train_time}, \"\n",
    "                    )\n",
    "\n",
    "                    # Save results\n",
    "                    hidden.append(hidden_size)\n",
    "                    layers.append(num_layer)\n",
    "                    swarm.append(swarm_size)\n",
    "                    epoch.append(epoch_num)\n",
    "                    batch.append(batch_size)\n",
    "                    times.append(train_time)\n",
    "\n",
    "# Output results as dataframe\n",
    "turba_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Hidden\": hidden,\n",
    "        \"Layers\": layers,\n",
    "        \"Swarm\": swarm,\n",
    "        \"Epoch\": epoch,\n",
    "        \"Batch\": batch,\n",
    "        \"Time\": times,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save timing data\n",
    "turba_data.to_csv(\"../../data/output/timing/turba_swarm_size_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
