{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from flax import linen\n",
    "from jax.lib import xla_bridge\n",
    "from turbanet import TurbaTrainState\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL INPUTS\n",
    "GPU = False\n",
    "repeats = 10\n",
    "\n",
    "# NETWORK SHAPE INPUTS\n",
    "hidden_sizes = [16]  # 8 * np.arange(1, 65)\n",
    "num_layers = [1]\n",
    "\n",
    "# TRAINING INPUTS\n",
    "lr = 1e-3\n",
    "dataset_size = 128\n",
    "swarm_sizes = 2 * np.arange(1, 65)\n",
    "epochs = [1024]\n",
    "batch_sizes = [128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinations: 64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Combinations: {len(hidden_sizes) * len(num_layers) * len(swarm_sizes) * len(epochs) * len(batch_sizes)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24ca2995890>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set numpy/torch/flax seeds\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random data\n",
    "def make_spirals(n_samples, noise_std=0.0, rotations=1.0):\n",
    "    ts = jnp.linspace(0, 1, n_samples)\n",
    "    rs = ts**0.5\n",
    "    thetas = rs * rotations * 2 * np.pi\n",
    "    signs = np.random.randint(0, 2, (n_samples,)) * 2 - 1\n",
    "    labels = (signs > 0).astype(int)\n",
    "\n",
    "    xs = rs * signs * jnp.cos(thetas) + np.random.randn(n_samples) * noise_std\n",
    "    ys = rs * signs * jnp.sin(thetas) + np.random.randn(n_samples) * noise_std\n",
    "    points = jnp.stack([xs, ys], axis=1)\n",
    "    return points, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, labels = make_spirals(dataset_size, noise_std=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_turba(params, input, output, apply_fn):\n",
    "    log_probs = apply_fn({\"params\": params}, input)\n",
    "    labels = jax.nn.one_hot(output, log_probs.shape[1])\n",
    "    loss = -jnp.mean(jnp.sum(labels * log_probs, axis=1))\n",
    "    return loss, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaxModel(linen.Module):\n",
    "    hidden_layers: int = 1\n",
    "    hidden_dim: int = 32\n",
    "\n",
    "    @linen.compact\n",
    "    def __call__(self, x):\n",
    "        for layer in range(self.hidden_layers):\n",
    "            x = linen.Dense(self.hidden_dim)(x)\n",
    "            x = linen.relu(x)\n",
    "        x = linen.Dense(2)(x)\n",
    "        x = linen.log_softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turba_train(turba_state, epochs, X, y):\n",
    "    for _ in range(epochs):\n",
    "        for batch_input, batch_label in zip(X, y):\n",
    "            turba_state, _, _ = turba_state.train(batch_input, batch_label, cross_entropy_turba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Nodes: 256, Layers: 2, Swarm: 2, Epochs: 1024, Batch: 128, Time: 2.1536871910095217, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 4, Epochs: 1024, Batch: 128, Time: 5.460443329811096, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 6, Epochs: 1024, Batch: 128, Time: 7.07486310005188, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 8, Epochs: 1024, Batch: 128, Time: 9.343785858154297, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 10, Epochs: 1024, Batch: 128, Time: 11.34181456565857, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 12, Epochs: 1024, Batch: 128, Time: 10.284096693992614, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 14, Epochs: 1024, Batch: 128, Time: 8.864269471168518, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 16, Epochs: 1024, Batch: 128, Time: 9.091983985900878, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 18, Epochs: 1024, Batch: 128, Time: 11.15328116416931, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 20, Epochs: 1024, Batch: 128, Time: 12.07243447303772, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 22, Epochs: 1024, Batch: 128, Time: 12.841281604766845, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 24, Epochs: 1024, Batch: 128, Time: 15.803068327903748, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 26, Epochs: 1024, Batch: 128, Time: 17.585751676559447, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 28, Epochs: 1024, Batch: 128, Time: 18.14292883872986, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 30, Epochs: 1024, Batch: 128, Time: 22.528338313102722, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 32, Epochs: 1024, Batch: 128, Time: 23.307802605628968, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 34, Epochs: 1024, Batch: 128, Time: 25.15512945652008, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 36, Epochs: 1024, Batch: 128, Time: 22.010552716255187, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 38, Epochs: 1024, Batch: 128, Time: 24.259368586540223, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 40, Epochs: 1024, Batch: 128, Time: 23.772075414657593, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 42, Epochs: 1024, Batch: 128, Time: 26.831972932815553, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 44, Epochs: 1024, Batch: 128, Time: 27.803391551971437, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 46, Epochs: 1024, Batch: 128, Time: 32.94092495441437, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 48, Epochs: 1024, Batch: 128, Time: 36.966634440422055, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 50, Epochs: 1024, Batch: 128, Time: 44.63331799507141, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 52, Epochs: 1024, Batch: 128, Time: 45.61475977897644, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 54, Epochs: 1024, Batch: 128, Time: 47.41527025699615, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 56, Epochs: 1024, Batch: 128, Time: 51.59439055919647, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 58, Epochs: 1024, Batch: 128, Time: 57.25227422714234, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 60, Epochs: 1024, Batch: 128, Time: 59.03913762569427, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 62, Epochs: 1024, Batch: 128, Time: 60.48179471492767, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 64, Epochs: 1024, Batch: 128, Time: 64.14589173793793, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 66, Epochs: 1024, Batch: 128, Time: 71.39283423423767, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 68, Epochs: 1024, Batch: 128, Time: 66.21101915836334, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 70, Epochs: 1024, Batch: 128, Time: 73.30571389198303, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 72, Epochs: 1024, Batch: 128, Time: 78.56516613960267, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 74, Epochs: 1024, Batch: 128, Time: 79.9287966966629, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 76, Epochs: 1024, Batch: 128, Time: 82.35451970100402, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 78, Epochs: 1024, Batch: 128, Time: 80.48016500473022, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 80, Epochs: 1024, Batch: 128, Time: 90.72275228500366, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 82, Epochs: 1024, Batch: 128, Time: 88.15516679286957, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 84, Epochs: 1024, Batch: 128, Time: 93.8352705001831, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 86, Epochs: 1024, Batch: 128, Time: 98.26873123645782, \n",
      "Hidden Nodes: 256, Layers: 2, Swarm: 88, Epochs: 1024, Batch: 128, Time: 103.82662901878356, \n"
     ]
    }
   ],
   "source": [
    "hidden = []\n",
    "layers = []\n",
    "swarm = []\n",
    "epoch = []\n",
    "batch = []\n",
    "times = []\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for num_layer in num_layers:\n",
    "        for swarm_size in swarm_sizes:\n",
    "            for epoch_num in epochs:\n",
    "                for batch_size in batch_sizes:\n",
    "                    # Create models\n",
    "                    turba_model = JaxModel(hidden_layers=num_layer, hidden_dim=hidden_size)\n",
    "                    turba_state = TurbaTrainState.swarm(\n",
    "                        turba_model, swarm_size, 2, learning_rate=lr\n",
    "                    )\n",
    "\n",
    "                    # Set Turba to use GPU if available\n",
    "                    if GPU and xla_bridge.get_backend().platform != \"gpu\":\n",
    "                        raise RuntimeError(\"GPU support not available for Turba.\")\n",
    "\n",
    "                    # Prepare data\n",
    "                    X_train_turba = jnp.array(\n",
    "                        np.expand_dims(points.reshape(-1, batch_size, 2), axis=1).repeat(\n",
    "                            swarm_size, axis=1\n",
    "                        ),\n",
    "                        dtype=jnp.float32,\n",
    "                    )\n",
    "                    y_train_turba = jnp.array(\n",
    "                        np.expand_dims(labels.reshape(-1, batch_size), axis=1).repeat(\n",
    "                            swarm_size, axis=1\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # Train\n",
    "                    train_times = np.zeros(repeats)\n",
    "                    for r in range(repeats):\n",
    "                        start = time()\n",
    "                        turba_train(turba_state, epoch_num, X_train_turba, y_train_turba)\n",
    "                        train_times[r] = time() - start\n",
    "\n",
    "                    train_time = train_times.mean()\n",
    "\n",
    "                    # Print results\n",
    "                    print(\n",
    "                        f\"Hidden Nodes: {hidden_size}, \"\n",
    "                        f\"Layers: {num_layer}, \"\n",
    "                        f\"Swarm: {swarm_size}, \"\n",
    "                        f\"Epochs: {epoch_num}, \"\n",
    "                        f\"Batch: {batch_size}, \"\n",
    "                        f\"Time: {train_time}, \"\n",
    "                    )\n",
    "\n",
    "                    # Save results\n",
    "                    hidden.append(hidden_size)\n",
    "                    layers.append(num_layer)\n",
    "                    swarm.append(swarm_size)\n",
    "                    epoch.append(epoch_num)\n",
    "                    batch.append(batch_size)\n",
    "                    times.append(train_time)\n",
    "\n",
    "# Output results as dataframe\n",
    "turba_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Hidden\": hidden,\n",
    "        \"Layers\": layers,\n",
    "        \"Swarm\": swarm,\n",
    "        \"Epoch\": epoch,\n",
    "        \"Batch\": batch,\n",
    "        \"Time\": times,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save timing data\n",
    "turba_data.to_csv(\"../../data/output/timing/turba_swarm_size_large_network_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchModel(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_layers: int):\n",
    "        super(TorchModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            *(nn.Linear(hidden_size, hidden_size), nn.ReLU()) * (num_layers - 1),\n",
    "            nn.Linear(hidden_size, 2),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_train(torch_models, torch_optimizers, epochs, X, y):\n",
    "    for torch_model, torch_optimizer in zip(torch_models, torch_optimizers):\n",
    "        torch_model.train()\n",
    "        for _ in range(epochs):\n",
    "            for batch_input, batch_label in zip(X, y):\n",
    "                torch_optimizer.zero_grad()\n",
    "                y_pred = torch_model(batch_input)\n",
    "                loss = torch.nn.functional.cross_entropy(y_pred, batch_label)\n",
    "                loss.backward()\n",
    "                torch_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Nodes: 16, Layers: 1, Swarm: 2, Epochs: 1024, Batch: 128, Time: 0.7277522563934327, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 4, Epochs: 1024, Batch: 128, Time: 1.4402210235595703, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 6, Epochs: 1024, Batch: 128, Time: 2.148959183692932, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 8, Epochs: 1024, Batch: 128, Time: 2.8256622552871704, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 10, Epochs: 1024, Batch: 128, Time: 3.536025357246399, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 12, Epochs: 1024, Batch: 128, Time: 4.2484043598175045, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 14, Epochs: 1024, Batch: 128, Time: 4.94863076210022, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 16, Epochs: 1024, Batch: 128, Time: 5.628749537467956, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 18, Epochs: 1024, Batch: 128, Time: 6.445216870307922, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 20, Epochs: 1024, Batch: 128, Time: 7.4354040145874025, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 22, Epochs: 1024, Batch: 128, Time: 8.366805744171142, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 24, Epochs: 1024, Batch: 128, Time: 9.386388540267944, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 26, Epochs: 1024, Batch: 128, Time: 10.155462312698365, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 28, Epochs: 1024, Batch: 128, Time: 10.997843265533447, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 30, Epochs: 1024, Batch: 128, Time: 11.679669213294982, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 32, Epochs: 1024, Batch: 128, Time: 12.545639371871948, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 34, Epochs: 1024, Batch: 128, Time: 13.288606142997741, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 36, Epochs: 1024, Batch: 128, Time: 13.973979163169862, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 38, Epochs: 1024, Batch: 128, Time: 14.840930342674255, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 40, Epochs: 1024, Batch: 128, Time: 15.564090991020203, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 42, Epochs: 1024, Batch: 128, Time: 16.325797963142396, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 44, Epochs: 1024, Batch: 128, Time: 17.162338733673096, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 46, Epochs: 1024, Batch: 128, Time: 17.895973801612854, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 48, Epochs: 1024, Batch: 128, Time: 18.650210571289062, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 50, Epochs: 1024, Batch: 128, Time: 19.513482189178468, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 52, Epochs: 1024, Batch: 128, Time: 20.279862928390504, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 54, Epochs: 1024, Batch: 128, Time: 21.04716284275055, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 56, Epochs: 1024, Batch: 128, Time: 21.808147525787355, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 58, Epochs: 1024, Batch: 128, Time: 22.523546981811524, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 60, Epochs: 1024, Batch: 128, Time: 23.347071409225464, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 62, Epochs: 1024, Batch: 128, Time: 24.139354991912843, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 64, Epochs: 1024, Batch: 128, Time: 25.016831517219543, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 66, Epochs: 1024, Batch: 128, Time: 25.718620014190673, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 68, Epochs: 1024, Batch: 128, Time: 26.426377058029175, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 70, Epochs: 1024, Batch: 128, Time: 27.156990575790406, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 72, Epochs: 1024, Batch: 128, Time: 28.009662556648255, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 74, Epochs: 1024, Batch: 128, Time: 28.85368402004242, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 76, Epochs: 1024, Batch: 128, Time: 29.534331178665163, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 78, Epochs: 1024, Batch: 128, Time: 30.47564239501953, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 80, Epochs: 1024, Batch: 128, Time: 31.10448546409607, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 82, Epochs: 1024, Batch: 128, Time: 31.92036988735199, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 84, Epochs: 1024, Batch: 128, Time: 32.73584311008453, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 86, Epochs: 1024, Batch: 128, Time: 33.47050549983978, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 88, Epochs: 1024, Batch: 128, Time: 34.46389663219452, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 90, Epochs: 1024, Batch: 128, Time: 35.084427046775815, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 92, Epochs: 1024, Batch: 128, Time: 35.8342925786972, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 94, Epochs: 1024, Batch: 128, Time: 36.67624371051788, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 96, Epochs: 1024, Batch: 128, Time: 37.38375301361084, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 98, Epochs: 1024, Batch: 128, Time: 38.27198810577393, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 100, Epochs: 1024, Batch: 128, Time: 39.087056279182434, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 102, Epochs: 1024, Batch: 128, Time: 39.93446612358093, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 104, Epochs: 1024, Batch: 128, Time: 40.681387186050415, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 106, Epochs: 1024, Batch: 128, Time: 41.36623640060425, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 108, Epochs: 1024, Batch: 128, Time: 42.671103262901305, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 110, Epochs: 1024, Batch: 128, Time: 42.91051013469696, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 112, Epochs: 1024, Batch: 128, Time: 43.67474458217621, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 114, Epochs: 1024, Batch: 128, Time: 44.571402764320375, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 116, Epochs: 1024, Batch: 128, Time: 45.386404156684875, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 118, Epochs: 1024, Batch: 128, Time: 46.02430872917175, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 120, Epochs: 1024, Batch: 128, Time: 47.03330075740814, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 122, Epochs: 1024, Batch: 128, Time: 46.32594125270843, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 124, Epochs: 1024, Batch: 128, Time: 48.44097397327423, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 126, Epochs: 1024, Batch: 128, Time: 49.50876219272614, \n",
      "Hidden Nodes: 16, Layers: 1, Swarm: 128, Epochs: 1024, Batch: 128, Time: 49.89313733577728, \n"
     ]
    }
   ],
   "source": [
    "hidden = []\n",
    "layers = []\n",
    "swarm = []\n",
    "epoch = []\n",
    "batch = []\n",
    "times = []\n",
    "losses = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    for num_layer in num_layers:\n",
    "        for swarm_size in swarm_sizes:\n",
    "            for epoch_num in epochs:\n",
    "                for batch_size in batch_sizes:\n",
    "                    # Create models\n",
    "                    torch_models = [TorchModel(hidden_size, num_layer) for _ in range(swarm_size)]\n",
    "\n",
    "                    # Create optimizers\n",
    "                    torch_optimizers = [\n",
    "                        torch.optim.Adam(torch_model.parameters(), lr=lr)\n",
    "                        for torch_model in torch_models\n",
    "                    ]\n",
    "\n",
    "                    # Set torch to use GPU if available\n",
    "                    device = torch.device(\"cpu\")\n",
    "                    if GPU and torch.cuda.is_available():\n",
    "                        device = torch.device(\"cuda\")\n",
    "                        for torch_model in torch_models:\n",
    "                            torch_model.to(device)\n",
    "\n",
    "                    # Prepare data\n",
    "                    X_train_torch = torch.from_numpy(\n",
    "                        np.array(points.reshape(-1, batch_size, 2))\n",
    "                    ).float()\n",
    "                    y_train_torch = torch.from_numpy(np.array(labels.reshape(-1, batch_size)))\n",
    "\n",
    "                    # Move to GPU if available\n",
    "                    if GPU:\n",
    "                        X_train_torch = X_train_torch.to(device)\n",
    "                        y_train_torch = y_train_torch.to(device)\n",
    "\n",
    "                    # Train\n",
    "                    train_times = np.zeros(repeats)\n",
    "                    for r in range(repeats):\n",
    "                        start = time()\n",
    "                        torch_train(\n",
    "                            torch_models, torch_optimizers, epoch_num, X_train_torch, y_train_torch\n",
    "                        )\n",
    "                        train_times[r] = time() - start\n",
    "\n",
    "                    train_time = train_times.mean()\n",
    "\n",
    "                    # Print results\n",
    "                    print(\n",
    "                        f\"Hidden Nodes: {hidden_size}, \"\n",
    "                        f\"Layers: {num_layer}, \"\n",
    "                        f\"Swarm: {swarm_size}, \"\n",
    "                        f\"Epochs: {epoch_num}, \"\n",
    "                        f\"Batch: {batch_size}, \"\n",
    "                        f\"Time: {train_time}, \"\n",
    "                    )\n",
    "\n",
    "                    # Save results\n",
    "                    hidden.append(hidden_size)\n",
    "                    layers.append(num_layer)\n",
    "                    swarm.append(swarm_size)\n",
    "                    epoch.append(epoch_num)\n",
    "                    batch.append(batch_size)\n",
    "                    times.append(train_time)\n",
    "\n",
    "# Output results as dataframe\n",
    "torch_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Hidden\": hidden,\n",
    "        \"Layers\": layers,\n",
    "        \"Swarm\": swarm,\n",
    "        \"Epoch\": epoch,\n",
    "        \"Batch\": batch,\n",
    "        \"Time\": times,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save timing data\n",
    "torch_data.to_csv(\"../../data/output/timing/torch_swarm_size_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
